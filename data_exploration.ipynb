{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "spark_jobs.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d-zg/Poker-Action-Classifier/blob/main/data_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRZIkSqT__Dv"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install spark. we are using the one that uses hadoop as the underlying scheduler.\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!tar xf  spark-3.2.4-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "#Provides findspark.init() to make pyspark importable as a regular library.\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.2.4-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'"
      ],
      "metadata": {
        "id": "_BcbpaYeKNlG",
        "outputId": "5670a513-eb72-4922-8414-dc3005e49040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 294136\n",
            "drwxr-xr-x  1 root root      4096 Apr 19 13:37 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr  9 21:17 spark-3.2.4-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 301183180 Apr  9 21:35 spark-3.2.4-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "xB4TPZH16jmR",
        "outputId": "3e9bd334-e75a-4519-d404-5f650bfb966e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write Spark Code Locally and test the Code and Save it to your repository"
      ],
      "metadata": {
        "id": "o1prQTAn7Mbu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz7dMIVJ__Dy"
      },
      "source": [
        "# Step 2. Complete Spark Jobs Below Locally. \n",
        "\n",
        "Once they work you can submit them to EMR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxUib2yt__Dy"
      },
      "source": [
        "## Job 1. Count the number of tweets.\n",
        "\n",
        "I have almost completed this for you. You still have to do the reduce and add - look into the wordcount example. But then use this as the template to finish the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDL78heZ__Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7b4618-08e2-429f-c04e-c7e72b31b17a"
      },
      "source": [
        "%%file schema.py\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR (? check proj)\n",
        "  conf = SparkConf().setAppName('schema').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "\n",
        "  try:\n",
        "    #@todo: fix the path as required\n",
        "    sqlContext = SQLContext(sc)\n",
        "\n",
        "    df = sqlContext.read.parquet('/content/nfd_incidents_xd_seg.parquet')\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    df.printSchema()\n",
        "    count_before = df.count()\n",
        "    # drop the rows that have empty column entries\n",
        "    df = df.dropna()\n",
        "\n",
        "    # count the number of rows after dropping empty rows\n",
        "    count_after = df.count()\n",
        "\n",
        "    # calculate the number of rows dropped\n",
        "    num_dropped = count_before - count_after\n",
        "\n",
        "    # print the count of rows before and after dropping empty rows\n",
        "    print(\"Number of rows before dropping empty rows: \", count_before)\n",
        "    print(\"Number of rows after dropping empty rows: \", count_after)\n",
        "\n",
        "    # print the number of rows dropped\n",
        "    print(\"Number of rows dropped: \", num_dropped)\n",
        "\n",
        "    print(\"Now printing unique columns\")\n",
        "    # loop through each column in the DataFrame\n",
        "    for column in df.columns:\n",
        "        # get the distinct values for the column\n",
        "        unique_values = df.select(column).distinct().limit(10)\n",
        "        \n",
        "        # print the column name and unique values\n",
        "        print(f\"Column: {column}\")\n",
        "        unique_values.show()\n",
        "        \n",
        "    # @todo: the s3 version will have to save it to correct s3 path\n",
        "    # output.repartition(1).saveAsTextFile(\"s3://vandy-bd/hw6/1_count.out\")\n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting schema.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test local Execution Results"
      ],
      "metadata": {
        "id": "PH2TFBJp7q0O"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3y2A34y__Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4320feb7-925b-4fce-a28c-01d7efe46938"
      },
      "source": [
        "# execute locally and ensure everything works. If it works you should get the 1_count.out/part-00000 file. \n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 schema.py\n",
        "# note the cell magic command %%file 1_count.py is used to create a local copy of the content of cell as a file 1_count.py on colab"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.4-bin-hadoop3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-190c0d46-7cb3-4675-b4b5-6b715e8d21a9;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 934ms :: artifacts dl 21ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-190c0d46-7cb3-4675-b4b5-6b715e8d21a9\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/29ms)\n",
            "23/04/22 18:10:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/22 18:10:50 INFO SparkContext: Running Spark version 3.2.4\n",
            "23/04/22 18:10:50 INFO ResourceUtils: ==============================================================\n",
            "23/04/22 18:10:50 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/22 18:10:50 INFO ResourceUtils: ==============================================================\n",
            "23/04/22 18:10:50 INFO SparkContext: Submitted application: schema\n",
            "23/04/22 18:10:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/22 18:10:50 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/22 18:10:50 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/22 18:10:50 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/22 18:10:50 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/22 18:10:50 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/22 18:10:50 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/22 18:10:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/22 18:10:51 INFO Utils: Successfully started service 'sparkDriver' on port 38979.\n",
            "23/04/22 18:10:51 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/22 18:10:51 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/22 18:10:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/22 18:10:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/22 18:10:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/22 18:10:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f77263d9-b494-4283-bad3-68b2dfad1357\n",
            "23/04/22 18:10:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/22 18:10:52 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/22 18:10:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/22 18:10:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://c93a18a26209:4040\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://c93a18a26209:38979/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://c93a18a26209:38979/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://c93a18a26209:38979/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://c93a18a26209:38979/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://c93a18a26209:38979/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://c93a18a26209:38979/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://c93a18a26209:38979/jars/com.101tec_zkclient-0.3.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://c93a18a26209:38979/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://c93a18a26209:38979/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://c93a18a26209:38979/jars/log4j_log4j-1.2.17.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://c93a18a26209:38979/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://c93a18a26209:38979/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/com.101tec_zkclient-0.3.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/log4j_log4j-1.2.17.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/22 18:10:53 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:53 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/22 18:10:54 INFO Executor: Starting executor ID driver on host c93a18a26209\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/log4j_log4j-1.2.17.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/com.101tec_zkclient-0.3.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/22 18:10:54 INFO Executor: Fetching spark://c93a18a26209:38979/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO TransportClientFactory: Successfully created connection to c93a18a26209/172.28.0.12:38979 after 130 ms (0 ms spent in bootstraps)\n",
            "23/04/22 18:10:54 INFO Utils: Fetching spark://c93a18a26209:38979/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp4929421993755499724.tmp\n",
            "23/04/22 18:10:54 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp4929421993755499724.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/22 18:10:54 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/22 18:10:54 INFO Executor: Fetching spark://c93a18a26209:38979/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: Fetching spark://c93a18a26209:38979/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp54110891489711446.tmp\n",
            "23/04/22 18:10:54 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp54110891489711446.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/22 18:10:54 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/22 18:10:54 INFO Executor: Fetching spark://c93a18a26209:38979/jars/com.101tec_zkclient-0.3.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: Fetching spark://c93a18a26209:38979/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp6446590768401965700.tmp\n",
            "23/04/22 18:10:54 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp6446590768401965700.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/com.101tec_zkclient-0.3.jar\n",
            "23/04/22 18:10:54 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/22 18:10:54 INFO Executor: Fetching spark://c93a18a26209:38979/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: Fetching spark://c93a18a26209:38979/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp394595171143850129.tmp\n",
            "23/04/22 18:10:54 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp394595171143850129.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/22 18:10:54 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/22 18:10:54 INFO Executor: Fetching spark://c93a18a26209:38979/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: Fetching spark://c93a18a26209:38979/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp2797025159258442487.tmp\n",
            "23/04/22 18:10:54 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp2797025159258442487.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/22 18:10:54 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/22 18:10:54 INFO Executor: Fetching spark://c93a18a26209:38979/jars/log4j_log4j-1.2.17.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: Fetching spark://c93a18a26209:38979/jars/log4j_log4j-1.2.17.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp64736712964863183.tmp\n",
            "23/04/22 18:10:54 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp64736712964863183.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/log4j_log4j-1.2.17.jar\n",
            "23/04/22 18:10:54 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/22 18:10:54 INFO Executor: Fetching spark://c93a18a26209:38979/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:54 INFO Utils: Fetching spark://c93a18a26209:38979/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp4982001076271628425.tmp\n",
            "23/04/22 18:10:54 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp4982001076271628425.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/22 18:10:55 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/22 18:10:55 INFO Executor: Fetching spark://c93a18a26209:38979/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:55 INFO Utils: Fetching spark://c93a18a26209:38979/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp3957880807805226156.tmp\n",
            "23/04/22 18:10:55 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp3957880807805226156.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/22 18:10:55 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/22 18:10:55 INFO Executor: Fetching spark://c93a18a26209:38979/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:55 INFO Utils: Fetching spark://c93a18a26209:38979/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp536238737495939025.tmp\n",
            "23/04/22 18:10:55 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp536238737495939025.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/22 18:10:55 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/22 18:10:55 INFO Executor: Fetching spark://c93a18a26209:38979/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:55 INFO Utils: Fetching spark://c93a18a26209:38979/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp6238042943861113222.tmp\n",
            "23/04/22 18:10:55 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp6238042943861113222.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/22 18:10:55 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/22 18:10:55 INFO Executor: Fetching spark://c93a18a26209:38979/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:55 INFO Utils: Fetching spark://c93a18a26209:38979/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp5146095300019889782.tmp\n",
            "23/04/22 18:10:55 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp5146095300019889782.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/22 18:10:55 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/22 18:10:55 INFO Executor: Fetching spark://c93a18a26209:38979/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1682187050623\n",
            "23/04/22 18:10:55 INFO Utils: Fetching spark://c93a18a26209:38979/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp275855825273684860.tmp\n",
            "23/04/22 18:10:55 INFO Utils: /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/fetchFileTemp275855825273684860.tmp has been previously copied to /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/22 18:10:55 INFO Executor: Adding file:/tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/userFiles-a785b3e2-7268-4fe4-b232-698c82b06e79/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/22 18:10:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41425.\n",
            "23/04/22 18:10:55 INFO NettyBlockTransferService: Server created on c93a18a26209:41425\n",
            "23/04/22 18:10:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/22 18:10:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c93a18a26209, 41425, None)\n",
            "23/04/22 18:10:55 INFO BlockManagerMasterEndpoint: Registering block manager c93a18a26209:41425 with 366.3 MiB RAM, BlockManagerId(driver, c93a18a26209, 41425, None)\n",
            "23/04/22 18:10:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c93a18a26209, 41425, None)\n",
            "23/04/22 18:10:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c93a18a26209, 41425, None)\n",
            "/content/spark-3.2.4-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "23/04/22 18:10:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "23/04/22 18:10:56 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/04/22 18:10:57 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.\n",
            "23/04/22 18:10:58 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:10:58 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:10:58 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:10:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:10:58 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:10:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:10:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 105.0 KiB, free 366.2 MiB)\n",
            "23/04/22 18:10:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 366.2 MiB)\n",
            "23/04/22 18:10:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c93a18a26209:41425 (size: 37.6 KiB, free: 366.3 MiB)\n",
            "23/04/22 18:10:58 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:10:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:10:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:10:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4590 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:10:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/22 18:11:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2231 bytes result sent to driver\n",
            "23/04/22 18:11:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1347 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:00 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.737 s\n",
            "23/04/22 18:11:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "23/04/22 18:11:00 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.856424 s\n",
            "23/04/22 18:11:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on c93a18a26209:41425 in memory (size: 37.6 KiB, free: 366.3 MiB)\n",
            "root\n",
            " |-- ID_Original: string (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- emdCardNumber: string (nullable = true)\n",
            " |-- time_utc: timestamp (nullable = true)\n",
            " |-- time_local: timestamp (nullable = true)\n",
            " |-- response_time_sec: double (nullable = true)\n",
            " |-- day_of_week: long (nullable = true)\n",
            " |-- weekend_or_not: integer (nullable = true)\n",
            " |-- geometry: string (nullable = true)\n",
            " |-- Incident_ID: integer (nullable = true)\n",
            " |-- Dist_to_Seg: double (nullable = true)\n",
            " |-- XDSegID: double (nullable = true)\n",
            "\n",
            "23/04/22 18:11:04 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:04 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "23/04/22 18:11:04 INFO FileSourceStrategy: Output Data Schema: struct<>\n",
            "23/04/22 18:11:04 INFO CodeGenerator: Code generated in 263.179984 ms\n",
            "23/04/22 18:11:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 352.2 KiB, free 366.0 MiB)\n",
            "23/04/22 18:11:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.5 KiB, free 365.9 MiB)\n",
            "23/04/22 18:11:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c93a18a26209:41425 (size: 35.5 KiB, free: 366.3 MiB)\n",
            "23/04/22 18:11:05 INFO SparkContext: Created broadcast 1 from count at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Registering RDD 5 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Got map stage job 1 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
            "23/04/22 18:11:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.9 MiB)\n",
            "23/04/22 18:11:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c93a18a26209:41425 (size: 7.2 KiB, free: 366.3 MiB)\n",
            "23/04/22 18:11:05 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/04/22 18:11:05 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2167 bytes result sent to driver\n",
            "23/04/22 18:11:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 422 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:05 INFO DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.507 s\n",
            "23/04/22 18:11:05 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:05 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:05 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:05 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:05 INFO CodeGenerator: Code generated in 41.269981 ms\n",
            "23/04/22 18:11:05 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.0 KiB, free 365.9 MiB)\n",
            "23/04/22 18:11:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/22 18:11:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c93a18a26209:41425 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/22 18:11:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:05 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
            "23/04/22 18:11:06 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 29 ms\n",
            "23/04/22 18:11:06 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2648 bytes result sent to driver\n",
            "23/04/22 18:11:06 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 211 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:06 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:06 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.243 s\n",
            "23/04/22 18:11:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/22 18:11:06 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 0.283017 s\n",
            "23/04/22 18:11:06 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:06 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:06 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:06 INFO CodeGenerator: Code generated in 176.376845 ms\n",
            "23/04/22 18:11:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 355.7 KiB, free 365.5 MiB)\n",
            "23/04/22 18:11:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 365.5 MiB)\n",
            "23/04/22 18:11:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:06 INFO SparkContext: Created broadcast 4 from count at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:06 INFO DAGScheduler: Registering RDD 12 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/22 18:11:06 INFO DAGScheduler: Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:06 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:06 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:06 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:06 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 23.3 KiB, free 365.5 MiB)\n",
            "23/04/22 18:11:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 365.5 MiB)\n",
            "23/04/22 18:11:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on c93a18a26209:41425 (size: 8.6 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:07 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:07 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:07 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:07 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
            "23/04/22 18:11:07 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:07 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "23/04/22 18:11:07 INFO BlockManagerInfo: Removed broadcast_3_piece0 on c93a18a26209:41425 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c93a18a26209:41425 in memory (size: 35.5 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:07 INFO BlockManagerInfo: Removed broadcast_2_piece0 on c93a18a26209:41425 in memory (size: 7.2 KiB, free: 366.3 MiB)\n",
            "23/04/22 18:11:08 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2223 bytes result sent to driver\n",
            "23/04/22 18:11:08 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 1442 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:08 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:08 INFO DAGScheduler: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 1.478 s\n",
            "23/04/22 18:11:08 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:08 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:08 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:08 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:08 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:08 INFO DAGScheduler: Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:08 INFO DAGScheduler: Final stage: ResultStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
            "23/04/22 18:11:08 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:08 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:08 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.0 KiB, free 365.9 MiB)\n",
            "23/04/22 18:11:08 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.9 MiB)\n",
            "23/04/22 18:11:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on c93a18a26209:41425 (size: 5.5 KiB, free: 366.3 MiB)\n",
            "23/04/22 18:11:08 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:08 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:08 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:08 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
            "23/04/22 18:11:08 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
            "23/04/22 18:11:08 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 2648 bytes result sent to driver\n",
            "23/04/22 18:11:08 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 22 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:08 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:08 INFO DAGScheduler: ResultStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.056 s\n",
            "23/04/22 18:11:08 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
            "23/04/22 18:11:08 INFO DAGScheduler: Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.070738 s\n",
            "Number of rows before dropping empty rows:  29765\n",
            "Number of rows after dropping empty rows:  21060\n",
            "Number of rows dropped:  8705\n",
            "Now printing unique columns\n",
            "Column: ID_Original\n",
            "23/04/22 18:11:09 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:09 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:09 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:09 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:09 INFO CodeGenerator: Code generated in 216.218341 ms\n",
            "23/04/22 18:11:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 355.7 KiB, free 365.5 MiB)\n",
            "23/04/22 18:11:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 365.5 MiB)\n",
            "23/04/22 18:11:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:09 INFO SparkContext: Created broadcast 7 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:09 INFO DAGScheduler: Registering RDD 19 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
            "23/04/22 18:11:09 INFO DAGScheduler: Got map stage job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:09 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:09 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:09 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:09 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[19] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:09 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.3 KiB, free 365.5 MiB)\n",
            "23/04/22 18:11:09 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:09 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on c93a18a26209:41425 (size: 12.4 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:09 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[19] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:09 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:09 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:09 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)\n",
            "23/04/22 18:11:09 INFO CodeGenerator: Code generated in 92.291956 ms\n",
            "23/04/22 18:11:10 INFO CodeGenerator: Code generated in 25.224154 ms\n",
            "23/04/22 18:11:10 INFO CodeGenerator: Code generated in 29.778467 ms\n",
            "23/04/22 18:11:10 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:10 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:10 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 751 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:10 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:10 INFO DAGScheduler: ShuffleMapStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 0.795 s\n",
            "23/04/22 18:11:10 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:10 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:10 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:10 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:10 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:10 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:10 INFO CodeGenerator: Code generated in 53.434176 ms\n",
            "23/04/22 18:11:10 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:10 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:10 INFO DAGScheduler: Final stage: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "23/04/22 18:11:10 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:10 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:10 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 33.7 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:10 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:10 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on c93a18a26209:41425 (size: 15.5 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:10 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:10 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:10 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:10 INFO Executor: Running task 0.0 in stage 9.0 (TID 6)\n",
            "23/04/22 18:11:10 INFO ShuffleBlockFetcherIterator: Getting 1 (413.2 KiB) non-empty blocks including 1 (413.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/22 18:11:10 INFO Executor: Finished task 0.0 in stage 9.0 (TID 6). 4166 bytes result sent to driver\n",
            "23/04/22 18:11:10 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 100 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:10 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:10 INFO DAGScheduler: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.118 s\n",
            "23/04/22 18:11:10 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "23/04/22 18:11:10 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.157012 s\n",
            "23/04/22 18:11:10 INFO CodeGenerator: Code generated in 16.14684 ms\n",
            "+--------------------+\n",
            "|         ID_Original|\n",
            "+--------------------+\n",
            "|ObjectId(59d3a823...|\n",
            "|ObjectId(59d3a83d...|\n",
            "|ObjectId(59d3a896...|\n",
            "|ObjectId(59d3a8be...|\n",
            "|ObjectId(59d3a943...|\n",
            "|ObjectId(59d3a9cf...|\n",
            "|ObjectId(59d3aa28...|\n",
            "|ObjectId(59d3aa77...|\n",
            "|ObjectId(59d3aa7e...|\n",
            "|ObjectId(59d3aa91...|\n",
            "+--------------------+\n",
            "\n",
            "Column: latitude\n",
            "23/04/22 18:11:11 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:11 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:11 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:11 INFO CodeGenerator: Code generated in 65.098532 ms\n",
            "23/04/22 18:11:11 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 355.7 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:11 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:11 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:11 INFO SparkContext: Created broadcast 10 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Registering RDD 26 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Got map stage job 7 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[26] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:11 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 38.7 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:11 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:11 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on c93a18a26209:41425 (size: 15.9 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:11 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[26] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:11 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:11 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 7) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:11 INFO Executor: Running task 0.0 in stage 10.0 (TID 7)\n",
            "23/04/22 18:11:11 INFO CodeGenerator: Code generated in 41.550638 ms\n",
            "23/04/22 18:11:11 INFO CodeGenerator: Code generated in 23.179388 ms\n",
            "23/04/22 18:11:11 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:11 INFO Executor: Finished task 0.0 in stage 10.0 (TID 7). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:11 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 7) in 433 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:11 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:11 INFO DAGScheduler: ShuffleMapStage 10 (showString at NativeMethodAccessorImpl.java:0) finished in 0.454 s\n",
            "23/04/22 18:11:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:11 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:11 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:11 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:11 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:11 INFO CodeGenerator: Code generated in 31.582671 ms\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Registering RDD 29 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Got map stage job 8 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:11 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 35.2 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:11 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:11 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on c93a18a26209:41425 (size: 16.5 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:11 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:11 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:11 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:11 INFO Executor: Running task 0.0 in stage 12.0 (TID 8)\n",
            "23/04/22 18:11:11 INFO ShuffleBlockFetcherIterator: Getting 1 (76.4 KiB) non-empty blocks including 1 (76.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/22 18:11:11 INFO Executor: Finished task 0.0 in stage 12.0 (TID 8). 4232 bytes result sent to driver\n",
            "23/04/22 18:11:11 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 38 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:11 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:11 INFO DAGScheduler: ShuffleMapStage 12 (showString at NativeMethodAccessorImpl.java:0) finished in 0.059 s\n",
            "23/04/22 18:11:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:11 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:11 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:11 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:12 INFO CodeGenerator: Code generated in 25.952399 ms\n",
            "23/04/22 18:11:12 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Got job 9 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Final stage: ResultStage 15 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[32] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:12 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 10.4 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:12 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on c93a18a26209:41425 (size: 5.2 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:12 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[32] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:12 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:12 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 9) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:12 INFO Executor: Running task 0.0 in stage 15.0 (TID 9)\n",
            "23/04/22 18:11:12 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/22 18:11:12 INFO Executor: Finished task 0.0 in stage 15.0 (TID 9). 2341 bytes result sent to driver\n",
            "23/04/22 18:11:12 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 9) in 17 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:12 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:12 INFO DAGScheduler: ResultStage 15 (showString at NativeMethodAccessorImpl.java:0) finished in 0.029 s\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Job 9 finished: showString at NativeMethodAccessorImpl.java:0, took 0.039990 s\n",
            "+-----------+\n",
            "|   latitude|\n",
            "+-----------+\n",
            "| 36.1648696|\n",
            "|36.20960059|\n",
            "|36.19310413|\n",
            "|36.14991684|\n",
            "|36.23756093|\n",
            "|36.15220408|\n",
            "| 36.1865875|\n",
            "|36.14045204|\n",
            "| 36.1567648|\n",
            "|36.14859459|\n",
            "+-----------+\n",
            "\n",
            "Column: longitude\n",
            "23/04/22 18:11:12 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:12 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:12 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:12 INFO CodeGenerator: Code generated in 70.829346 ms\n",
            "23/04/22 18:11:12 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 355.7 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:12 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Removed broadcast_13_piece0 on c93a18a26209:41425 in memory (size: 5.2 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:12 INFO SparkContext: Created broadcast 14 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Registering RDD 36 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Got map stage job 10 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:12 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 38.7 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:12 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on c93a18a26209:41425 (size: 15.9 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:12 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[36] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:12 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:12 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 10) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:12 INFO Executor: Running task 0.0 in stage 16.0 (TID 10)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:12 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Removed broadcast_11_piece0 on c93a18a26209:41425 in memory (size: 15.9 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on c93a18a26209:41425 in memory (size: 5.5 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Removed broadcast_8_piece0 on c93a18a26209:41425 in memory (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Removed broadcast_10_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on c93a18a26209:41425 in memory (size: 15.5 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Removed broadcast_12_piece0 on c93a18a26209:41425 in memory (size: 16.5 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:12 INFO Executor: Finished task 0.0 in stage 16.0 (TID 10). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:12 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 10) in 326 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:12 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:12 INFO DAGScheduler: ShuffleMapStage 16 (showString at NativeMethodAccessorImpl.java:0) finished in 0.349 s\n",
            "23/04/22 18:11:12 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:12 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:12 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:12 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:12 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:12 INFO CodeGenerator: Code generated in 32.252415 ms\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Registering RDD 39 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Got map stage job 11 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[39] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:12 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 35.2 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:12 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on c93a18a26209:41425 (size: 16.5 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:12 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[39] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:12 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:12 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 11) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:12 INFO Executor: Running task 0.0 in stage 18.0 (TID 11)\n",
            "23/04/22 18:11:12 INFO ShuffleBlockFetcherIterator: Getting 1 (75.7 KiB) non-empty blocks including 1 (75.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/22 18:11:12 INFO Executor: Finished task 0.0 in stage 18.0 (TID 11). 4232 bytes result sent to driver\n",
            "23/04/22 18:11:12 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 11) in 49 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:12 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:12 INFO DAGScheduler: ShuffleMapStage 18 (showString at NativeMethodAccessorImpl.java:0) finished in 0.061 s\n",
            "23/04/22 18:11:12 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:12 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:12 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:12 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:13 INFO CodeGenerator: Code generated in 16.331971 ms\n",
            "23/04/22 18:11:13 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Got job 12 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Final stage: ResultStage 21 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[42] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:13 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 10.4 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:13 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:13 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on c93a18a26209:41425 (size: 5.2 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:13 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[42] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:13 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:13 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 12) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:13 INFO Executor: Running task 0.0 in stage 21.0 (TID 12)\n",
            "23/04/22 18:11:13 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:13 INFO Executor: Finished task 0.0 in stage 21.0 (TID 12). 2296 bytes result sent to driver\n",
            "23/04/22 18:11:13 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 12) in 12 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:13 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:13 INFO DAGScheduler: ResultStage 21 (showString at NativeMethodAccessorImpl.java:0) finished in 0.027 s\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Job 12 finished: showString at NativeMethodAccessorImpl.java:0, took 0.034831 s\n",
            "+------------+\n",
            "|   longitude|\n",
            "+------------+\n",
            "|-86.79073489|\n",
            "|-86.64799528|\n",
            "|-86.87032497|\n",
            "|-86.77471487|\n",
            "|-86.78262053|\n",
            "|-86.76844889|\n",
            "|-86.69016503|\n",
            "|-86.72504417|\n",
            "|-86.78165187|\n",
            "|-86.74927197|\n",
            "+------------+\n",
            "\n",
            "Column: emdCardNumber\n",
            "23/04/22 18:11:13 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:13 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:13 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:13 INFO CodeGenerator: Code generated in 46.89899 ms\n",
            "23/04/22 18:11:13 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 355.7 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:13 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:13 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:13 INFO SparkContext: Created broadcast 18 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Registering RDD 46 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Got map stage job 13 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[46] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:13 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 32.3 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:13 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on c93a18a26209:41425 (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:13 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[46] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:13 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:13 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 13) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:13 INFO Executor: Running task 0.0 in stage 22.0 (TID 13)\n",
            "23/04/22 18:11:13 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:13 INFO Executor: Finished task 0.0 in stage 22.0 (TID 13). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:13 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 13) in 270 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:13 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:13 INFO DAGScheduler: ShuffleMapStage 22 (showString at NativeMethodAccessorImpl.java:0) finished in 0.301 s\n",
            "23/04/22 18:11:13 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:13 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:13 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:13 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:13 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:13 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Got job 14 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Final stage: ResultStage 24 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[49] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:13 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 33.8 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:13 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:13 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on c93a18a26209:41425 (size: 15.5 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:13 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:13 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:13 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 14) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:13 INFO Executor: Running task 0.0 in stage 24.0 (TID 14)\n",
            "23/04/22 18:11:13 INFO ShuffleBlockFetcherIterator: Getting 1 (5.2 KiB) non-empty blocks including 1 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "23/04/22 18:11:13 INFO Executor: Finished task 0.0 in stage 24.0 (TID 14). 4085 bytes result sent to driver\n",
            "23/04/22 18:11:13 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 14) in 47 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:13 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:13 INFO DAGScheduler: ResultStage 24 (showString at NativeMethodAccessorImpl.java:0) finished in 0.083 s\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
            "23/04/22 18:11:13 INFO DAGScheduler: Job 14 finished: showString at NativeMethodAccessorImpl.java:0, took 0.099184 s\n",
            "+-------------+\n",
            "|emdCardNumber|\n",
            "+-------------+\n",
            "|        29B5V|\n",
            "|        29D2N|\n",
            "|        29A2Y|\n",
            "|         29O1|\n",
            "|        29D6U|\n",
            "|        29B2U|\n",
            "|        29D6Y|\n",
            "|        29D5X|\n",
            "|        29A1U|\n",
            "|        29D8Y|\n",
            "+-------------+\n",
            "\n",
            "Column: time_utc\n",
            "23/04/22 18:11:13 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:13 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:13 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:14 INFO CodeGenerator: Code generated in 53.301168 ms\n",
            "23/04/22 18:11:14 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 355.7 KiB, free 364.6 MiB)\n",
            "23/04/22 18:11:14 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:14 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:14 INFO SparkContext: Created broadcast 21 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Registering RDD 53 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 8\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Got map stage job 15 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[53] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:14 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.3 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:14 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:14 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on c93a18a26209:41425 (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:14 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[53] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:14 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:14 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 15) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:14 INFO Executor: Running task 0.0 in stage 25.0 (TID 15)\n",
            "23/04/22 18:11:14 INFO CodeGenerator: Code generated in 16.456685 ms\n",
            "23/04/22 18:11:14 INFO CodeGenerator: Code generated in 16.606552 ms\n",
            "23/04/22 18:11:14 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:14 INFO Executor: Finished task 0.0 in stage 25.0 (TID 15). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:14 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 15) in 462 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:14 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:14 INFO DAGScheduler: ShuffleMapStage 25 (showString at NativeMethodAccessorImpl.java:0) finished in 0.473 s\n",
            "23/04/22 18:11:14 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:14 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:14 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:14 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:14 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:14 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:14 INFO CodeGenerator: Code generated in 50.13475 ms\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Registering RDD 56 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 9\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Got map stage job 16 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[56] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:14 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 34.5 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:14 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:14 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on c93a18a26209:41425 (size: 16.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:14 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[56] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:14 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:14 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 16) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:14 INFO Executor: Running task 0.0 in stage 27.0 (TID 16)\n",
            "23/04/22 18:11:14 INFO ShuffleBlockFetcherIterator: Getting 1 (194.8 KiB) non-empty blocks including 1 (194.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/22 18:11:14 INFO Executor: Finished task 0.0 in stage 27.0 (TID 16). 4232 bytes result sent to driver\n",
            "23/04/22 18:11:14 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 16) in 42 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:14 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:14 INFO DAGScheduler: ShuffleMapStage 27 (showString at NativeMethodAccessorImpl.java:0) finished in 0.061 s\n",
            "23/04/22 18:11:14 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:14 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:14 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:14 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:14 INFO CodeGenerator: Code generated in 9.707577 ms\n",
            "23/04/22 18:11:14 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Got job 17 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Final stage: ResultStage 30 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[59] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:14 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 11.3 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:14 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:14 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on c93a18a26209:41425 (size: 5.7 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:14 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[59] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:14 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:14 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 17) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:14 INFO Executor: Running task 0.0 in stage 30.0 (TID 17)\n",
            "23/04/22 18:11:14 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:14 INFO Executor: Finished task 0.0 in stage 30.0 (TID 17). 2369 bytes result sent to driver\n",
            "23/04/22 18:11:14 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 17) in 18 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:14 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:14 INFO DAGScheduler: ResultStage 30 (showString at NativeMethodAccessorImpl.java:0) finished in 0.028 s\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished\n",
            "23/04/22 18:11:14 INFO DAGScheduler: Job 17 finished: showString at NativeMethodAccessorImpl.java:0, took 0.037120 s\n",
            "+--------------------+\n",
            "|            time_utc|\n",
            "+--------------------+\n",
            "| 2017-01-30 18:26:12|\n",
            "| 2017-03-07 08:55:53|\n",
            "| 2017-03-07 13:03:32|\n",
            "| 2017-03-17 13:57:09|\n",
            "|2017-03-20 15:24:...|\n",
            "| 2017-04-07 17:18:48|\n",
            "| 2017-04-12 17:17:46|\n",
            "| 2017-04-23 00:26:20|\n",
            "| 2017-05-15 09:12:46|\n",
            "| 2017-05-25 15:00:08|\n",
            "+--------------------+\n",
            "\n",
            "Column: time_local\n",
            "23/04/22 18:11:14 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:14 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:14 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:14 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:14 INFO BlockManagerInfo: Removed broadcast_22_piece0 on c93a18a26209:41425 in memory (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Removed broadcast_19_piece0 on c93a18a26209:41425 in memory (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:15 INFO CodeGenerator: Code generated in 67.886157 ms\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Removed broadcast_16_piece0 on c93a18a26209:41425 in memory (size: 16.5 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Removed broadcast_14_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 355.7 KiB, free 364.6 MiB)\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 364.6 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:15 INFO SparkContext: Created broadcast 25 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Removed broadcast_18_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Registering RDD 63 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 10\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Got map stage job 18 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[63] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.3 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Removed broadcast_20_piece0 on c93a18a26209:41425 in memory (size: 15.5 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on c93a18a26209:41425 (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:15 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[63] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:15 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:15 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 18) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:15 INFO Executor: Running task 0.0 in stage 31.0 (TID 18)\n",
            "23/04/22 18:11:15 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Removed broadcast_23_piece0 on c93a18a26209:41425 in memory (size: 16.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Removed broadcast_17_piece0 on c93a18a26209:41425 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Removed broadcast_24_piece0 on c93a18a26209:41425 in memory (size: 5.7 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Removed broadcast_15_piece0 on c93a18a26209:41425 in memory (size: 15.9 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Removed broadcast_21_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:15 INFO Executor: Finished task 0.0 in stage 31.0 (TID 18). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:15 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 18) in 276 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:15 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:15 INFO DAGScheduler: ShuffleMapStage 31 (showString at NativeMethodAccessorImpl.java:0) finished in 0.299 s\n",
            "23/04/22 18:11:15 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:15 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:15 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:15 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:15 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:15 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:15 INFO CodeGenerator: Code generated in 25.314857 ms\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Registering RDD 66 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 11\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Got map stage job 19 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Final stage: ShuffleMapStage 33 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[66] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 34.5 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on c93a18a26209:41425 (size: 16.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:15 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[66] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:15 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:15 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 19) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:15 INFO Executor: Running task 0.0 in stage 33.0 (TID 19)\n",
            "23/04/22 18:11:15 INFO ShuffleBlockFetcherIterator: Getting 1 (195.5 KiB) non-empty blocks including 1 (195.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/22 18:11:15 INFO Executor: Finished task 0.0 in stage 33.0 (TID 19). 4232 bytes result sent to driver\n",
            "23/04/22 18:11:15 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 19) in 54 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:15 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:15 INFO DAGScheduler: ShuffleMapStage 33 (showString at NativeMethodAccessorImpl.java:0) finished in 0.070 s\n",
            "23/04/22 18:11:15 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:15 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:15 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:15 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:15 INFO CodeGenerator: Code generated in 15.537623 ms\n",
            "23/04/22 18:11:15 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Got job 20 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Final stage: ResultStage 36 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[69] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 11.3 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on c93a18a26209:41425 (size: 5.7 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:15 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[69] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:15 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:15 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 20) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:15 INFO Executor: Running task 0.0 in stage 36.0 (TID 20)\n",
            "23/04/22 18:11:15 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:15 INFO Executor: Finished task 0.0 in stage 36.0 (TID 20). 2424 bytes result sent to driver\n",
            "23/04/22 18:11:15 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 20) in 19 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:15 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:15 INFO DAGScheduler: ResultStage 36 (showString at NativeMethodAccessorImpl.java:0) finished in 0.028 s\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Job 20 finished: showString at NativeMethodAccessorImpl.java:0, took 0.039236 s\n",
            "+--------------------+\n",
            "|          time_local|\n",
            "+--------------------+\n",
            "|2017-01-12 21:06:...|\n",
            "| 2017-02-10 05:42:56|\n",
            "|2017-02-11 15:46:...|\n",
            "| 2017-03-19 06:35:28|\n",
            "| 2017-03-20 09:48:07|\n",
            "| 2017-04-06 04:13:41|\n",
            "|2017-04-17 08:15:...|\n",
            "| 2017-06-01 11:00:23|\n",
            "|2018-07-13 14:20:...|\n",
            "|2018-07-18 13:09:...|\n",
            "+--------------------+\n",
            "\n",
            "Column: response_time_sec\n",
            "23/04/22 18:11:15 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:15 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:15 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:15 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:15 INFO CodeGenerator: Code generated in 34.793453 ms\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 355.7 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:15 INFO SparkContext: Created broadcast 29 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Registering RDD 73 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 12\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Got map stage job 21 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Final stage: ShuffleMapStage 37 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[73] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 38.7 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:15 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:15 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on c93a18a26209:41425 (size: 15.9 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:15 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[73] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:15 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:15 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 21) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:15 INFO Executor: Running task 0.0 in stage 37.0 (TID 21)\n",
            "23/04/22 18:11:15 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:16 INFO Executor: Finished task 0.0 in stage 37.0 (TID 21). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 21) in 189 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:16 INFO DAGScheduler: ShuffleMapStage 37 (showString at NativeMethodAccessorImpl.java:0) finished in 0.198 s\n",
            "23/04/22 18:11:16 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:16 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:16 INFO ShufflePartitionsUtil: For shuffle(12), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:16 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:16 INFO CodeGenerator: Code generated in 17.737039 ms\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Registering RDD 76 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 13\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Got map stage job 22 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Final stage: ShuffleMapStage 39 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Submitting ShuffleMapStage 39 (MapPartitionsRDD[76] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 35.2 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:16 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on c93a18a26209:41425 (size: 16.5 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:16 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 39 (MapPartitionsRDD[76] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 22) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:16 INFO Executor: Running task 0.0 in stage 39.0 (TID 22)\n",
            "23/04/22 18:11:16 INFO ShuffleBlockFetcherIterator: Getting 1 (17.9 KiB) non-empty blocks including 1 (17.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:16 INFO Executor: Finished task 0.0 in stage 39.0 (TID 22). 4232 bytes result sent to driver\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 22) in 30 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:16 INFO DAGScheduler: ShuffleMapStage 39 (showString at NativeMethodAccessorImpl.java:0) finished in 0.042 s\n",
            "23/04/22 18:11:16 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:16 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:16 INFO CodeGenerator: Code generated in 10.35834 ms\n",
            "23/04/22 18:11:16 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Got job 23 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Final stage: ResultStage 42 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[79] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 10.4 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:16 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on c93a18a26209:41425 (size: 5.2 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:16 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[79] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 23) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:16 INFO Executor: Running task 0.0 in stage 42.0 (TID 23)\n",
            "23/04/22 18:11:16 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:16 INFO Executor: Finished task 0.0 in stage 42.0 (TID 23). 2258 bytes result sent to driver\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 23) in 14 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:16 INFO DAGScheduler: ResultStage 42 (showString at NativeMethodAccessorImpl.java:0) finished in 0.021 s\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Job 23 finished: showString at NativeMethodAccessorImpl.java:0, took 0.024266 s\n",
            "+-----------------+\n",
            "|response_time_sec|\n",
            "+-----------------+\n",
            "|            299.0|\n",
            "|            305.0|\n",
            "|            496.0|\n",
            "|            596.0|\n",
            "|            558.0|\n",
            "|            692.0|\n",
            "|            934.0|\n",
            "|            769.0|\n",
            "|           1051.0|\n",
            "|            170.0|\n",
            "+-----------------+\n",
            "\n",
            "Column: day_of_week\n",
            "23/04/22 18:11:16 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:16 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:16 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:16 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:16 INFO CodeGenerator: Code generated in 33.450874 ms\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 355.7 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:16 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:16 INFO SparkContext: Created broadcast 33 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Registering RDD 83 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 14\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Got map stage job 24 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Final stage: ShuffleMapStage 43 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[83] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 32.3 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:16 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on c93a18a26209:41425 (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:16 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[83] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 24) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:16 INFO Executor: Running task 0.0 in stage 43.0 (TID 24)\n",
            "23/04/22 18:11:16 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:16 INFO Executor: Finished task 0.0 in stage 43.0 (TID 24). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 24) in 164 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:16 INFO DAGScheduler: ShuffleMapStage 43 (showString at NativeMethodAccessorImpl.java:0) finished in 0.176 s\n",
            "23/04/22 18:11:16 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:16 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:16 INFO ShufflePartitionsUtil: For shuffle(14), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:16 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:16 INFO CodeGenerator: Code generated in 25.543539 ms\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Registering RDD 86 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 15\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Got map stage job 25 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Final stage: ShuffleMapStage 45 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[86] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 34.5 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:16 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on c93a18a26209:41425 (size: 16.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:16 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[86] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 25) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:16 INFO Executor: Running task 0.0 in stage 45.0 (TID 25)\n",
            "23/04/22 18:11:16 INFO ShuffleBlockFetcherIterator: Getting 1 (420.0 B) non-empty blocks including 1 (420.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:16 INFO Executor: Finished task 0.0 in stage 45.0 (TID 25). 4232 bytes result sent to driver\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 25) in 21 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:16 INFO DAGScheduler: ShuffleMapStage 45 (showString at NativeMethodAccessorImpl.java:0) finished in 0.030 s\n",
            "23/04/22 18:11:16 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:16 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:16 INFO CodeGenerator: Code generated in 12.2442 ms\n",
            "23/04/22 18:11:16 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Got job 26 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Final stage: ResultStage 48 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 47)\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[89] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 10.4 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:16 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on c93a18a26209:41425 (size: 5.2 KiB, free: 366.0 MiB)\n",
            "23/04/22 18:11:16 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[89] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 26) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:16 INFO Executor: Running task 0.0 in stage 48.0 (TID 26)\n",
            "23/04/22 18:11:16 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:16 INFO Executor: Finished task 0.0 in stage 48.0 (TID 26). 2548 bytes result sent to driver\n",
            "23/04/22 18:11:16 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 26) in 11 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:16 INFO DAGScheduler: ResultStage 48 (showString at NativeMethodAccessorImpl.java:0) finished in 0.022 s\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished\n",
            "23/04/22 18:11:16 INFO DAGScheduler: Job 26 finished: showString at NativeMethodAccessorImpl.java:0, took 0.025980 s\n",
            "+-----------+\n",
            "|day_of_week|\n",
            "+-----------+\n",
            "|          0|\n",
            "|          6|\n",
            "|          5|\n",
            "|          1|\n",
            "|          3|\n",
            "|          2|\n",
            "|          4|\n",
            "+-----------+\n",
            "\n",
            "Column: weekend_or_not\n",
            "23/04/22 18:11:16 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:16 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:16 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:16 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:16 INFO BlockManagerInfo: Removed broadcast_36_piece0 on c93a18a26209:41425 in memory (size: 5.2 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:16 INFO BlockManagerInfo: Removed broadcast_30_piece0 on c93a18a26209:41425 in memory (size: 15.9 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:16 INFO CodeGenerator: Code generated in 31.938108 ms\n",
            "23/04/22 18:11:16 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 355.7 KiB, free 364.1 MiB)\n",
            "23/04/22 18:11:16 INFO BlockManagerInfo: Removed broadcast_32_piece0 on c93a18a26209:41425 in memory (size: 5.2 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 364.1 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.0 MiB)\n",
            "23/04/22 18:11:17 INFO SparkContext: Created broadcast 37 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Removed broadcast_25_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Registering RDD 93 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 16\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Got map stage job 27 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Final stage: ShuffleMapStage 49 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Submitting ShuffleMapStage 49 (MapPartitionsRDD[93] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 32.3 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on c93a18a26209:41425 (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:17 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[93] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:17 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 27) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Removed broadcast_31_piece0 on c93a18a26209:41425 in memory (size: 16.5 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:17 INFO Executor: Running task 0.0 in stage 49.0 (TID 27)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Removed broadcast_28_piece0 on c93a18a26209:41425 in memory (size: 5.7 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:17 INFO CodeGenerator: Code generated in 12.517403 ms\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Removed broadcast_27_piece0 on c93a18a26209:41425 in memory (size: 16.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:17 INFO CodeGenerator: Code generated in 17.880481 ms\n",
            "23/04/22 18:11:17 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Removed broadcast_26_piece0 on c93a18a26209:41425 in memory (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Removed broadcast_33_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Removed broadcast_34_piece0 on c93a18a26209:41425 in memory (size: 12.4 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Removed broadcast_35_piece0 on c93a18a26209:41425 in memory (size: 16.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Removed broadcast_29_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:17 INFO Executor: Finished task 0.0 in stage 49.0 (TID 27). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:17 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 27) in 201 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:17 INFO DAGScheduler: ShuffleMapStage 49 (showString at NativeMethodAccessorImpl.java:0) finished in 0.211 s\n",
            "23/04/22 18:11:17 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:17 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:17 INFO ShufflePartitionsUtil: For shuffle(16), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:17 INFO CodeGenerator: Code generated in 13.901572 ms\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Registering RDD 96 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 17\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Got map stage job 28 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Final stage: ShuffleMapStage 51 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 50)\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[96] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 34.5 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on c93a18a26209:41425 (size: 16.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:17 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[96] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:17 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 28) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:17 INFO Executor: Running task 0.0 in stage 51.0 (TID 28)\n",
            "23/04/22 18:11:17 INFO ShuffleBlockFetcherIterator: Getting 1 (120.0 B) non-empty blocks including 1 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:17 INFO Executor: Finished task 0.0 in stage 51.0 (TID 28). 4232 bytes result sent to driver\n",
            "23/04/22 18:11:17 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 28) in 22 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:17 INFO DAGScheduler: ShuffleMapStage 51 (showString at NativeMethodAccessorImpl.java:0) finished in 0.033 s\n",
            "23/04/22 18:11:17 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:17 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:17 INFO CodeGenerator: Code generated in 18.898451 ms\n",
            "23/04/22 18:11:17 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Got job 29 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Final stage: ResultStage 54 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 53)\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[99] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 10.4 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on c93a18a26209:41425 (size: 5.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:17 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[99] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:17 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 29) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:17 INFO Executor: Running task 0.0 in stage 54.0 (TID 29)\n",
            "23/04/22 18:11:17 INFO ShuffleBlockFetcherIterator: Getting 1 (66.0 B) non-empty blocks including 1 (66.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:17 INFO Executor: Finished task 0.0 in stage 54.0 (TID 29). 2498 bytes result sent to driver\n",
            "23/04/22 18:11:17 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 29) in 11 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:17 INFO DAGScheduler: ResultStage 54 (showString at NativeMethodAccessorImpl.java:0) finished in 0.025 s\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Job 29 finished: showString at NativeMethodAccessorImpl.java:0, took 0.029232 s\n",
            "+--------------+\n",
            "|weekend_or_not|\n",
            "+--------------+\n",
            "|             1|\n",
            "|             0|\n",
            "+--------------+\n",
            "\n",
            "Column: geometry\n",
            "23/04/22 18:11:17 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:17 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:17 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:17 INFO CodeGenerator: Code generated in 23.258799 ms\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 355.7 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:17 INFO SparkContext: Created broadcast 41 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Registering RDD 103 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 18\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Got map stage job 30 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Final stage: ShuffleMapStage 55 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Submitting ShuffleMapStage 55 (MapPartitionsRDD[103] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 32.3 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on c93a18a26209:41425 (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:17 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 55 (MapPartitionsRDD[103] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:17 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 30) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:17 INFO Executor: Running task 0.0 in stage 55.0 (TID 30)\n",
            "23/04/22 18:11:17 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:17 INFO Executor: Finished task 0.0 in stage 55.0 (TID 30). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:17 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 30) in 200 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:17 INFO DAGScheduler: ShuffleMapStage 55 (showString at NativeMethodAccessorImpl.java:0) finished in 0.212 s\n",
            "23/04/22 18:11:17 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:17 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:17 INFO ShufflePartitionsUtil: For shuffle(18), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:17 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Got job 31 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Final stage: ResultStage 57 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 56)\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[106] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 33.8 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:17 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 15.6 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:17 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on c93a18a26209:41425 (size: 15.6 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:17 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[106] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:17 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 31) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:17 INFO Executor: Running task 0.0 in stage 57.0 (TID 31)\n",
            "23/04/22 18:11:17 INFO ShuffleBlockFetcherIterator: Getting 1 (186.8 KiB) non-empty blocks including 1 (186.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:17 INFO Executor: Finished task 0.0 in stage 57.0 (TID 31). 4249 bytes result sent to driver\n",
            "23/04/22 18:11:17 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 31) in 31 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:17 INFO DAGScheduler: ResultStage 57 (showString at NativeMethodAccessorImpl.java:0) finished in 0.046 s\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished\n",
            "23/04/22 18:11:17 INFO DAGScheduler: Job 31 finished: showString at NativeMethodAccessorImpl.java:0, took 0.053796 s\n",
            "+--------------------+\n",
            "|            geometry|\n",
            "+--------------------+\n",
            "|POINT (-86.724444...|\n",
            "|POINT (-86.749936...|\n",
            "|POINT (-86.765351...|\n",
            "|POINT (-86.870324...|\n",
            "|POINT (-86.618477...|\n",
            "|POINT (-86.712987...|\n",
            "|POINT (-86.739919...|\n",
            "|POINT (-86.775773...|\n",
            "|POINT (-86.620928...|\n",
            "|POINT (-86.778581...|\n",
            "+--------------------+\n",
            "\n",
            "Column: Incident_ID\n",
            "23/04/22 18:11:17 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:17 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:17 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:18 INFO CodeGenerator: Code generated in 21.696999 ms\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 355.7 KiB, free 364.6 MiB)\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO SparkContext: Created broadcast 44 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Registering RDD 110 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 19\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Got map stage job 32 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Final stage: ShuffleMapStage 58 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Submitting ShuffleMapStage 58 (MapPartitionsRDD[110] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 32.3 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 12.4 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on c93a18a26209:41425 (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[110] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:18 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:18 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 32) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:18 INFO Executor: Running task 0.0 in stage 58.0 (TID 32)\n",
            "23/04/22 18:11:18 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:18 INFO Executor: Finished task 0.0 in stage 58.0 (TID 32). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:18 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 32) in 211 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:18 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:18 INFO DAGScheduler: ShuffleMapStage 58 (showString at NativeMethodAccessorImpl.java:0) finished in 0.221 s\n",
            "23/04/22 18:11:18 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:18 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:18 INFO ShufflePartitionsUtil: For shuffle(19), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:18 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:18 INFO CodeGenerator: Code generated in 30.872005 ms\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Registering RDD 113 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 20\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Got map stage job 33 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Final stage: ShuffleMapStage 60 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 59)\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Submitting ShuffleMapStage 60 (MapPartitionsRDD[113] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 34.5 KiB, free 364.5 MiB)\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on c93a18a26209:41425 (size: 16.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[113] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:18 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:18 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 33) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:18 INFO Executor: Running task 0.0 in stage 60.0 (TID 33)\n",
            "23/04/22 18:11:18 INFO ShuffleBlockFetcherIterator: Getting 1 (201.2 KiB) non-empty blocks including 1 (201.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:18 INFO Executor: Finished task 0.0 in stage 60.0 (TID 33). 4232 bytes result sent to driver\n",
            "23/04/22 18:11:18 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 33) in 46 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:18 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:18 INFO DAGScheduler: ShuffleMapStage 60 (showString at NativeMethodAccessorImpl.java:0) finished in 0.057 s\n",
            "23/04/22 18:11:18 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:18 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:18 INFO CodeGenerator: Code generated in 31.882596 ms\n",
            "23/04/22 18:11:18 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Got job 34 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Final stage: ResultStage 63 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 62)\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[116] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 10.4 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 364.4 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on c93a18a26209:41425 (size: 5.2 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[116] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:18 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:18 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 34) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:18 INFO Executor: Running task 0.0 in stage 63.0 (TID 34)\n",
            "23/04/22 18:11:18 INFO ShuffleBlockFetcherIterator: Getting 1 (142.0 B) non-empty blocks including 1 (142.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:18 INFO Executor: Finished task 0.0 in stage 63.0 (TID 34). 2257 bytes result sent to driver\n",
            "23/04/22 18:11:18 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 34) in 13 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:18 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:18 INFO DAGScheduler: ResultStage 63 (showString at NativeMethodAccessorImpl.java:0) finished in 0.023 s\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Job 34 finished: showString at NativeMethodAccessorImpl.java:0, took 0.027625 s\n",
            "+-----------+\n",
            "|Incident_ID|\n",
            "+-----------+\n",
            "|        148|\n",
            "|        463|\n",
            "|        471|\n",
            "|        496|\n",
            "|        833|\n",
            "|       1088|\n",
            "|       1238|\n",
            "|       1342|\n",
            "|       1580|\n",
            "|       1591|\n",
            "+-----------+\n",
            "\n",
            "Column: Dist_to_Seg\n",
            "23/04/22 18:11:18 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:18 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:18 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:18 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:18 INFO CodeGenerator: Code generated in 50.431328 ms\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 355.7 KiB, free 364.1 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_40_piece0 on c93a18a26209:41425 in memory (size: 5.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_38_piece0 on c93a18a26209:41425 in memory (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_41_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_45_piece0 on c93a18a26209:41425 in memory (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_39_piece0 on c93a18a26209:41425 in memory (size: 16.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 364.6 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO SparkContext: Created broadcast 48 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_47_piece0 on c93a18a26209:41425 in memory (size: 5.2 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_43_piece0 on c93a18a26209:41425 in memory (size: 15.6 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Registering RDD 120 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 21\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Got map stage job 35 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Final stage: ShuffleMapStage 64 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Submitting ShuffleMapStage 64 (MapPartitionsRDD[120] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_42_piece0 on c93a18a26209:41425 in memory (size: 12.4 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 38.7 KiB, free 364.7 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_44_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:18 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 365.1 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on c93a18a26209:41425 (size: 15.9 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:18 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 64 (MapPartitionsRDD[120] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:18 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:18 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 35) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:18 INFO Executor: Running task 0.0 in stage 64.0 (TID 35)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_46_piece0 on c93a18a26209:41425 in memory (size: 16.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:18 INFO BlockManagerInfo: Removed broadcast_37_piece0 on c93a18a26209:41425 in memory (size: 36.1 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:18 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:18 INFO Executor: Finished task 0.0 in stage 64.0 (TID 35). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:18 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 35) in 180 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:18 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:18 INFO DAGScheduler: ShuffleMapStage 64 (showString at NativeMethodAccessorImpl.java:0) finished in 0.201 s\n",
            "23/04/22 18:11:18 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:18 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:18 INFO ShufflePartitionsUtil: For shuffle(21), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:18 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:18 INFO CodeGenerator: Code generated in 22.935036 ms\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Registering RDD 123 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 22\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Got map stage job 36 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Final stage: ShuffleMapStage 66 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 65)\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:18 INFO DAGScheduler: Submitting ShuffleMapStage 66 (MapPartitionsRDD[123] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 35.2 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:19 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on c93a18a26209:41425 (size: 16.5 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:19 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 66 (MapPartitionsRDD[123] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:19 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 36) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:19 INFO Executor: Running task 0.0 in stage 66.0 (TID 36)\n",
            "23/04/22 18:11:19 INFO ShuffleBlockFetcherIterator: Getting 1 (86.5 KiB) non-empty blocks including 1 (86.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "23/04/22 18:11:19 INFO Executor: Finished task 0.0 in stage 66.0 (TID 36). 4232 bytes result sent to driver\n",
            "23/04/22 18:11:19 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 36) in 26 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:19 INFO DAGScheduler: ShuffleMapStage 66 (showString at NativeMethodAccessorImpl.java:0) finished in 0.034 s\n",
            "23/04/22 18:11:19 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:19 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:19 INFO CodeGenerator: Code generated in 13.389337 ms\n",
            "23/04/22 18:11:19 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Got job 37 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Final stage: ResultStage 69 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 68)\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[126] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 10.5 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 365.4 MiB)\n",
            "23/04/22 18:11:19 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on c93a18a26209:41425 (size: 5.2 KiB, free: 366.2 MiB)\n",
            "23/04/22 18:11:19 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[126] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:19 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 37) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:19 INFO Executor: Running task 0.0 in stage 69.0 (TID 37)\n",
            "23/04/22 18:11:19 INFO ShuffleBlockFetcherIterator: Getting 1 (156.0 B) non-empty blocks including 1 (156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:19 INFO Executor: Finished task 0.0 in stage 69.0 (TID 37). 2447 bytes result sent to driver\n",
            "23/04/22 18:11:19 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 37) in 10 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:19 INFO DAGScheduler: ResultStage 69 (showString at NativeMethodAccessorImpl.java:0) finished in 0.023 s\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Job 37 finished: showString at NativeMethodAccessorImpl.java:0, took 0.026724 s\n",
            "+-------------------+\n",
            "|        Dist_to_Seg|\n",
            "+-------------------+\n",
            "|  4.931653435167389|\n",
            "| 0.7031145601466819|\n",
            "| 0.7190442838616523|\n",
            "| 1.6309729923261462|\n",
            "| 1.5965826710249482|\n",
            "|  8.683544450601971|\n",
            "| 10.884654275114348|\n",
            "|0.21383410109805298|\n",
            "| 1.3510398291171508|\n",
            "|0.47652164418043147|\n",
            "+-------------------+\n",
            "\n",
            "Column: XDSegID\n",
            "23/04/22 18:11:19 INFO FileSourceStrategy: Pushed Filters: \n",
            "23/04/22 18:11:19 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(13, ID_Original#0, latitude#1, longitude#2, emdCardNumber#3, time_utc#4, time_local#5, response_time_sec#6, day_of_week#7L, weekend_or_not#8, geometry#9, Incident_ID#10, Dist_to_Seg#11, XDSegID#12)\n",
            "23/04/22 18:11:19 INFO FileSourceStrategy: Output Data Schema: struct<ID_Original: string, latitude: double, longitude: double, emdCardNumber: string, time_utc: timestamp ... 11 more fields>\n",
            "23/04/22 18:11:19 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:19 INFO CodeGenerator: Code generated in 29.822048 ms\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 355.7 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:19 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on c93a18a26209:41425 (size: 36.1 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:19 INFO SparkContext: Created broadcast 52 from showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Registering RDD 130 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 23\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Got map stage job 38 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Final stage: ShuffleMapStage 70 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Parents of final stage: List()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Submitting ShuffleMapStage 70 (MapPartitionsRDD[130] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 38.7 KiB, free 365.0 MiB)\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:19 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on c93a18a26209:41425 (size: 15.9 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:19 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 70 (MapPartitionsRDD[130] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:19 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 38) (c93a18a26209, executor driver, partition 0, PROCESS_LOCAL, 4858 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:19 INFO Executor: Running task 0.0 in stage 70.0 (TID 38)\n",
            "23/04/22 18:11:19 INFO FileScanRDD: Reading File path: file:///content/nfd_incidents_xd_seg.parquet, range: 0-1975841, partition values: [empty row]\n",
            "23/04/22 18:11:19 INFO Executor: Finished task 0.0 in stage 70.0 (TID 38). 2983 bytes result sent to driver\n",
            "23/04/22 18:11:19 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 38) in 191 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:19 INFO DAGScheduler: ShuffleMapStage 70 (showString at NativeMethodAccessorImpl.java:0) finished in 0.204 s\n",
            "23/04/22 18:11:19 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:19 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:19 INFO ShufflePartitionsUtil: For shuffle(23), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "23/04/22 18:11:19 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "23/04/22 18:11:19 INFO CodeGenerator: Code generated in 14.387924 ms\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Registering RDD 133 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 24\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Got map stage job 39 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Final stage: ShuffleMapStage 72 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 71)\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Submitting ShuffleMapStage 72 (MapPartitionsRDD[133] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 35.2 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:19 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on c93a18a26209:41425 (size: 16.5 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:19 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 72 (MapPartitionsRDD[133] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:19 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 39) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:19 INFO Executor: Running task 0.0 in stage 72.0 (TID 39)\n",
            "23/04/22 18:11:19 INFO ShuffleBlockFetcherIterator: Getting 1 (31.8 KiB) non-empty blocks including 1 (31.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
            "23/04/22 18:11:19 INFO Executor: Finished task 0.0 in stage 72.0 (TID 39). 4232 bytes result sent to driver\n",
            "23/04/22 18:11:19 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 39) in 39 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:19 INFO DAGScheduler: ShuffleMapStage 72 (showString at NativeMethodAccessorImpl.java:0) finished in 0.051 s\n",
            "23/04/22 18:11:19 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/22 18:11:19 INFO DAGScheduler: running: Set()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: waiting: Set()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: failed: Set()\n",
            "23/04/22 18:11:19 INFO CodeGenerator: Code generated in 15.581392 ms\n",
            "23/04/22 18:11:19 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Got job 40 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Final stage: ResultStage 75 (showString at NativeMethodAccessorImpl.java:0)\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 74)\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Missing parents: List()\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[136] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 10.5 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:19 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 364.9 MiB)\n",
            "23/04/22 18:11:19 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on c93a18a26209:41425 (size: 5.2 KiB, free: 366.1 MiB)\n",
            "23/04/22 18:11:19 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1474\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[136] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0\n",
            "23/04/22 18:11:19 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 40) (c93a18a26209, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
            "23/04/22 18:11:19 INFO Executor: Running task 0.0 in stage 75.0 (TID 40)\n",
            "23/04/22 18:11:19 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/22 18:11:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "23/04/22 18:11:19 INFO Executor: Finished task 0.0 in stage 75.0 (TID 40). 2336 bytes result sent to driver\n",
            "23/04/22 18:11:19 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 40) in 23 ms on c93a18a26209 (executor driver) (1/1)\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool \n",
            "23/04/22 18:11:19 INFO DAGScheduler: ResultStage 75 (showString at NativeMethodAccessorImpl.java:0) finished in 0.032 s\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/22 18:11:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished\n",
            "23/04/22 18:11:19 INFO DAGScheduler: Job 40 finished: showString at NativeMethodAccessorImpl.java:0, took 0.040417 s\n",
            "+-------------+\n",
            "|      XDSegID|\n",
            "+-------------+\n",
            "|1.524470198E9|\n",
            "| 1.52446673E9|\n",
            "| 4.41552617E8|\n",
            "|  4.5042845E8|\n",
            "|1.524475884E9|\n",
            "|1.524609417E9|\n",
            "| 4.29334846E8|\n",
            "|1.524322123E9|\n",
            "| 4.49620825E8|\n",
            "| 4.49621869E8|\n",
            "+-------------+\n",
            "\n",
            "23/04/22 18:11:19 INFO SparkUI: Stopped Spark web UI at http://c93a18a26209:4040\n",
            "23/04/22 18:11:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/22 18:11:19 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/22 18:11:19 INFO BlockManager: BlockManager stopped\n",
            "23/04/22 18:11:19 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/22 18:11:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/22 18:11:19 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/22 18:11:20 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/22 18:11:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1/pyspark-2178c486-daf1-4d79-a2e1-49e451c1625e\n",
            "23/04/22 18:11:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-0b42def8-2d40-4c6c-a75e-62009604dfb7\n",
            "23/04/22 18:11:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f833fe8-8cee-444c-9305-cdc4dfd4ddd1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9ZZT06__Dy"
      },
      "source": [
        "### Please save the output of each job as a single text file into your S3 bucket.\n",
        "\n",
        "Hint:\n",
        "\n",
        "1. You may call the **saveAsTextFile** function to populate the output file. \n",
        "2. Note spark may generate multiple output files due to partitioning, you can use the **repartition** or **coalesce** function to merge them to a single one.\n",
        "\n",
        "**You need to replace all s3 uri shown in below cells with yours.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNPsauvU__Dz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuvRtrzh__Dz"
      },
      "source": [
        "# upload script to S3. This assumes that your bucket name is vandy-bigdata. if not then change the  paths here.\n",
        "s3.upload_file(Filename='1_count.py', Bucket='vandy-bd', Key='hw6/1_count.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aGPm_qa__Dz"
      },
      "source": [
        "# submit spark job to emr. Make all the necessary changes to the path\n",
        "submit_job(app_name='1_count', pyfile_uri='s3://vandy-bd/hw6/1_count.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXU9mXRv__D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0241c8-0faa-4513-bf11-84eb7c4cf76a"
      },
      "source": [
        "# test EMR execution results. Once again, make sure that S3 paths are consistent.\n",
        "output_key = \"hw6/1_count.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bd', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test1(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BPCSIi0__D0"
      },
      "source": [
        "## Job 2. Count the screen name with the most tweets and its counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvw5wFdq__D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8efc31-6913-4c45-c7ce-9ca13ba18230"
      },
      "source": [
        "%%file schema.py\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "#create spark context. This is very important. Do this similarly for the other parts\n",
        "# Note to read a file directly from s3 into an rdd you may have to do something like this\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # replace this line with the s3 pass when testing over EMR (? check proj)\n",
        "  conf = SparkConf().setAppName('schema').set('spark.hadoop.validateOutputSpecs', False)\n",
        "  sc = SparkContext(conf=conf).getOrCreate()\n",
        "\n",
        "  try:\n",
        "    #@todo: fix the path as required\n",
        "    sqlContext = SQLContext(sc)\n",
        "\n",
        "    df = sqlContext.read.parquet('/content/nfd_incidents_xd_seg.parquet')\n",
        "    # review the page rank example for how to use the map operation\n",
        "    # review word count for reduce and add\n",
        "    # see how we use map to parse each row\n",
        "    df.printSchema()\n",
        "    count_before = df.count()\n",
        "    # drop the rows that have empty column entries\n",
        "    df = df.dropna()\n",
        "\n",
        "    # count the number of rows after dropping empty rows\n",
        "    count_after = df.count()\n",
        "\n",
        "    # calculate the number of rows dropped\n",
        "    num_dropped = count_before - count_after\n",
        "\n",
        "    # print the count of rows before and after dropping empty rows\n",
        "    print(\"Number of rows before dropping empty rows: \", count_before)\n",
        "    print(\"Number of rows after dropping empty rows: \", count_after)\n",
        "\n",
        "    # print the number of rows dropped\n",
        "    print(\"Number of rows dropped: \", num_dropped)\n",
        "\n",
        "    # @todo: the s3 version will have to save it to correct s3 path\n",
        "    # output.repartition(1).saveAsTextFile(\"s3://vandy-bd/hw6/1_count.out\")\n",
        "\n",
        "  finally:\n",
        "    # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "    #finally is used to make sure the context is stopped even with errors\n",
        "    sc.stop()\n",
        "  \n",
        "\n",
        " \n",
        "  \n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 2_group.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute and test locally"
      ],
      "metadata": {
        "id": "1xe2ZqP18Uso"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH6TK-Nk__D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fafa7d-099a-4500-d889-ff0c52eadc2c"
      },
      "source": [
        "\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 2_group.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-07408ff3-55fc-46dc-9a6f-b67ff36fc606;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 784ms :: artifacts dl 33ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-07408ff3-55fc-46dc-9a6f-b67ff36fc606\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/21ms)\n",
            "23/04/11 17:47:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/11 17:47:58 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/11 17:47:59 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 17:47:59 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/11 17:47:59 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 17:47:59 INFO SparkContext: Submitted application: 2_group\n",
            "23/04/11 17:47:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/11 17:47:59 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/11 17:47:59 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/11 17:47:59 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/11 17:47:59 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/11 17:47:59 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/11 17:47:59 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/11 17:47:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/11 17:47:59 INFO Utils: Successfully started service 'sparkDriver' on port 34497.\n",
            "23/04/11 17:47:59 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/11 17:47:59 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/11 17:47:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/11 17:47:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/11 17:47:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/11 17:47:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ddc062b7-2fc2-47fa-a4a7-b3e947c77bd3\n",
            "23/04/11 17:47:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/11 17:47:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/11 17:48:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/11 17:48:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ed3eecdcf213:4040\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://ed3eecdcf213:34497/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://ed3eecdcf213:34497/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://ed3eecdcf213:34497/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://ed3eecdcf213:34497/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://ed3eecdcf213:34497/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://ed3eecdcf213:34497/jars/log4j_log4j-1.2.17.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://ed3eecdcf213:34497/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://ed3eecdcf213:34497/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:48:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:48:00 INFO Executor: Starting executor ID driver on host ed3eecdcf213\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:00 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:48:00 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO TransportClientFactory: Successfully created connection to ed3eecdcf213/172.28.0.12:34497 after 59 ms (0 ms spent in bootstraps)\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3084373140676113814.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3084373140676113814.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp2079614968253254215.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp2079614968253254215.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp4006233683702446428.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp4006233683702446428.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp8490424229472509464.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp8490424229472509464.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1253708418988046172.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1253708418988046172.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp4385509867363552970.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp4385509867363552970.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3217525169282415113.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3217525169282415113.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3390783527352272984.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3390783527352272984.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1487910310076018304.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1487910310076018304.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/log4j_log4j-1.2.17.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/log4j_log4j-1.2.17.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp6083834070089690532.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp6083834070089690532.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3346549257331982440.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp3346549257331982440.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/11 17:48:01 INFO Executor: Fetching spark://ed3eecdcf213:34497/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235278970\n",
            "23/04/11 17:48:01 INFO Utils: Fetching spark://ed3eecdcf213:34497/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1303077820120664359.tmp\n",
            "23/04/11 17:48:01 INFO Utils: /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/fetchFileTemp1303077820120664359.tmp has been previously copied to /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:48:01 INFO Executor: Adding file:/tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/userFiles-08e2d55f-c201-476c-b03a-7b8085783adc/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/11 17:48:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39279.\n",
            "23/04/11 17:48:01 INFO NettyBlockTransferService: Server created on ed3eecdcf213:39279\n",
            "23/04/11 17:48:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/11 17:48:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ed3eecdcf213, 39279, None)\n",
            "23/04/11 17:48:01 INFO BlockManagerMasterEndpoint: Registering block manager ed3eecdcf213:39279 with 366.3 MiB RAM, BlockManagerId(driver, ed3eecdcf213, 39279, None)\n",
            "23/04/11 17:48:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ed3eecdcf213, 39279, None)\n",
            "23/04/11 17:48:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ed3eecdcf213, 39279, None)\n",
            "23/04/11 17:48:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/11 17:48:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ed3eecdcf213:39279 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:48:02 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/11 17:48:02 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/11 17:48:03 INFO SparkContext: Starting job: reduce at /content/2_group.py:38\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/2_group.py:37) as input to shuffle 0\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Got job 0 (reduce at /content/2_group.py:38) with 2 output partitions\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Final stage: ResultStage 1 (reduce at /content/2_group.py:38)\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/2_group.py:37), which has no missing parents\n",
            "23/04/11 17:48:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ed3eecdcf213:39279 (size: 7.6 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:48:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:48:03 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/2_group.py:37) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/11 17:48:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "23/04/11 17:48:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ed3eecdcf213, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ed3eecdcf213, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/11 17:48:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/11 17:48:04 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/11 17:48:04 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/11 17:48:07 INFO PythonRunner: Times: total = 2190, boot = 1322, init = 61, finish = 807\n",
            "23/04/11 17:48:07 INFO PythonRunner: Times: total = 2326, boot = 1322, init = 65, finish = 939\n",
            "23/04/11 17:48:07 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1612 bytes result sent to driver\n",
            "23/04/11 17:48:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1655 bytes result sent to driver\n",
            "23/04/11 17:48:07 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4129 ms on ed3eecdcf213 (executor driver) (1/2)\n",
            "23/04/11 17:48:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4229 ms on ed3eecdcf213 (executor driver) (2/2)\n",
            "23/04/11 17:48:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:48:07 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 35103\n",
            "23/04/11 17:48:08 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/2_group.py:37) finished in 4.729 s\n",
            "23/04/11 17:48:08 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 17:48:08 INFO DAGScheduler: running: Set()\n",
            "23/04/11 17:48:08 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at reduce at /content/2_group.py:38), which has no missing parents\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.6 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ed3eecdcf213:39279 (size: 6.2 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:48:08 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at reduce at /content/2_group.py:38) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (ed3eecdcf213, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (ed3eecdcf213, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "23/04/11 17:48:08 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Getting 2 (21.2 KiB) non-empty blocks including 2 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 40 ms\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Getting 2 (21.2 KiB) non-empty blocks including 2 (21.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 50 ms\n",
            "23/04/11 17:48:08 INFO PythonRunner: Times: total = 23, boot = -918, init = 937, finish = 4\n",
            "23/04/11 17:48:08 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1623 bytes result sent to driver\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 212 ms on ed3eecdcf213 (executor driver) (1/2)\n",
            "23/04/11 17:48:08 INFO PythonRunner: Times: total = 53, boot = -775, init = 825, finish = 3\n",
            "23/04/11 17:48:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1627 bytes result sent to driver\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 243 ms on ed3eecdcf213 (executor driver) (2/2)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: ResultStage 1 (reduce at /content/2_group.py:38) finished in 0.271 s\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:48:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Job 0 finished: reduce at /content/2_group.py:38, took 5.174936 s\n",
            "23/04/11 17:48:08 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/11 17:48:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 17:48:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 17:48:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 17:48:08 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Registering RDD 9 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Got job 1 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.8 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 365.9 MiB)\n",
            "23/04/11 17:48:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ed3eecdcf213:39279 (size: 4.8 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:48:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (ed3eecdcf213, executor driver, partition 0, PROCESS_LOCAL, 4422 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5) (ed3eecdcf213, executor driver, partition 1, PROCESS_LOCAL, 4460 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)\n",
            "23/04/11 17:48:08 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)\n",
            "23/04/11 17:48:08 INFO PythonRunner: Times: total = 44, boot = -353, init = 397, finish = 0\n",
            "23/04/11 17:48:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1353 bytes result sent to driver\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 103 ms on ed3eecdcf213 (executor driver) (1/2)\n",
            "23/04/11 17:48:08 INFO PythonRunner: Times: total = 53, boot = -365, init = 418, finish = 0\n",
            "23/04/11 17:48:08 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 1482 bytes result sent to driver\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 122 ms on ed3eecdcf213 (executor driver) (2/2)\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:48:08 INFO DAGScheduler: ShuffleMapStage 2 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.148 s\n",
            "23/04/11 17:48:08 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 17:48:08 INFO DAGScheduler: running: Set()\n",
            "23/04/11 17:48:08 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
            "23/04/11 17:48:08 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 105.9 KiB, free 365.8 MiB)\n",
            "23/04/11 17:48:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.7 MiB)\n",
            "23/04/11 17:48:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ed3eecdcf213:39279 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/11 17:48:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:48:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/11 17:48:08 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "23/04/11 17:48:08 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6) (ed3eecdcf213, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:48:08 INFO Executor: Running task 0.0 in stage 3.0 (TID 6)\n",
            "23/04/11 17:48:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 17:48:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 17:48:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:48:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "23/04/11 17:48:09 INFO PythonRunner: Times: total = 43, boot = -204, init = 247, finish = 0\n",
            "23/04/11 17:48:09 INFO FileOutputCommitter: Saved output of task 'attempt_202304111748087265173027041852046_0015_m_000000_0' to file:/content/2_group.out/_temporary/0/task_202304111748087265173027041852046_0015_m_000000\n",
            "23/04/11 17:48:09 INFO SparkHadoopMapRedUtil: attempt_202304111748087265173027041852046_0015_m_000000_0: Committed\n",
            "23/04/11 17:48:09 INFO Executor: Finished task 0.0 in stage 3.0 (TID 6). 1952 bytes result sent to driver\n",
            "23/04/11 17:48:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 258 ms on ed3eecdcf213 (executor driver) (1/1)\n",
            "23/04/11 17:48:09 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:48:09 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 0.298 s\n",
            "23/04/11 17:48:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/11 17:48:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "23/04/11 17:48:09 INFO DAGScheduler: Job 1 finished: runJob at SparkHadoopWriter.scala:83, took 0.473611 s\n",
            "23/04/11 17:48:09 INFO SparkHadoopWriter: Start to commit write Job job_202304111748087265173027041852046_0015.\n",
            "23/04/11 17:48:09 INFO SparkHadoopWriter: Write Job job_202304111748087265173027041852046_0015 committed. Elapsed time: 24 ms.\n",
            "23/04/11 17:48:09 INFO SparkUI: Stopped Spark web UI at http://ed3eecdcf213:4040\n",
            "23/04/11 17:48:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/11 17:48:09 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/11 17:48:09 INFO BlockManager: BlockManager stopped\n",
            "23/04/11 17:48:09 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/11 17:48:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/11 17:48:09 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/11 17:48:10 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/11 17:48:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700\n",
            "23/04/11 17:48:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-f868edd5-9f26-414b-bcb3-7531bedfa700/pyspark-52414ed4-6994-4c85-ac93-04d8b80ce72d\n",
            "23/04/11 17:48:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-1f8501ae-2a1b-453f-a70d-626f08a21a42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvpqT474__D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2dc793-ed3c-496e-8c58-08caf13877e3"
      },
      "source": [
        "def test2(lines):\n",
        "    assert lines[0].strip() == \"('rpsabo', 88)\"\n",
        "    print(\"passed\")\n",
        "\n",
        "# test local execution results\n",
        "with open('2_group.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  test2(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeAaRlMR__D1"
      },
      "source": [
        "## Job 3. Count the tweets per day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTMTHj9O__D2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6df42d-06cd-4424-94e3-1478d838b3f1"
      },
      "source": [
        "%%file 3_days.py\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "'''\n",
        "TODO:\n",
        "Count the tweets per day.\n",
        "\n",
        "See http://mike.teczno.com/notes/streaming-data-from-twitter.html for parsing info.\n",
        "Get the screen name by accessing tweet['user']['screen_name']\n",
        "\n",
        "Look at tweet['created_at'] for datetime of creation. Just use the first word in the date to get the day.\n",
        "\n",
        "'''\n",
        "\n",
        "def process(entry):\n",
        "  try:\n",
        "    tweet = json.loads(entry)\n",
        "    #if load succeeded. We use correct as the key\n",
        "    name = tweet['created_at'].split(' ')[0]\n",
        "    return name, 1\n",
        "  except:\n",
        "    #there was an error in loading. We use incorrect as the key\n",
        "    return \"incorrect\", 0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    conf = SparkConf().setAppName('3_days').set('spark.hadoop.validateOutputSpecs', False)\n",
        "    sc = SparkContext(conf=conf).getOrCreate()\n",
        "    try:\n",
        "      #@todo: fix the path as required\n",
        "      tweets=sc.textFile('s3://vandy-bd/hw6/nashville-tweets-2019-01-28')\n",
        "      # review the page rank example for how to use the map operation\n",
        "      # review word count for reduce and add\n",
        "      # see how we use map to parse each row\n",
        "      tweetinfo = tweets.map(lambda tweet: process(tweet))\n",
        "\n",
        "      # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "      counts = tweetinfo.reduceByKey(lambda x, y: x + y)\n",
        "      # @todo: the s3 version will have to save it to correct s3 path\n",
        "      counts.repartition(1).saveAsTextFile(\"s3://vandy-bd/hw6/3_days.out\")\n",
        "\n",
        "    finally:\n",
        "      # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "      #finally is used to make sure the context is stopped even with errors\n",
        "      sc.stop()\n",
        "\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 3_days.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6phVcUW__D2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5f876a-ea0d-408e-8cb0-7c75e44f2582"
      },
      "source": [
        "# execute locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 3_days.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-58659cb3-689d-45dd-9bb7-c1e75c13e278;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 881ms :: artifacts dl 42ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-58659cb3-689d-45dd-9bb7-c1e75c13e278\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/29ms)\n",
            "23/04/11 17:53:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/11 17:53:45 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/11 17:53:45 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 17:53:45 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/11 17:53:45 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 17:53:45 INFO SparkContext: Submitted application: 3_days\n",
            "23/04/11 17:53:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/11 17:53:45 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/11 17:53:45 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/11 17:53:45 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/11 17:53:45 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/11 17:53:45 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/11 17:53:45 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/11 17:53:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/11 17:53:45 INFO Utils: Successfully started service 'sparkDriver' on port 44007.\n",
            "23/04/11 17:53:45 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/11 17:53:45 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/11 17:53:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/11 17:53:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/11 17:53:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/11 17:53:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-48d66f4a-bc2f-4889-8e52-8e26c22a9f56\n",
            "23/04/11 17:53:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/11 17:53:46 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/11 17:53:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/11 17:53:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ed3eecdcf213:4040\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://ed3eecdcf213:44007/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://ed3eecdcf213:44007/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://ed3eecdcf213:44007/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://ed3eecdcf213:44007/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://ed3eecdcf213:44007/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://ed3eecdcf213:44007/jars/log4j_log4j-1.2.17.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://ed3eecdcf213:44007/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://ed3eecdcf213:44007/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:53:46 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:46 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:53:47 INFO Executor: Starting executor ID driver on host ed3eecdcf213\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO TransportClientFactory: Successfully created connection to ed3eecdcf213/172.28.0.12:44007 after 49 ms (0 ms spent in bootstraps)\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp5790867608299968887.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp5790867608299968887.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp251204340706217194.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp251204340706217194.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp9079337244795208535.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp9079337244795208535.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp9111849829727596329.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp9111849829727596329.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp8761562424887191527.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp8761562424887191527.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp4680667975855305434.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp4680667975855305434.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp6497805972133494473.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp6497805972133494473.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp1601756906431295054.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp1601756906431295054.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/log4j_log4j-1.2.17.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/log4j_log4j-1.2.17.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp6295988508873001202.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp6295988508873001202.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/log4j_log4j-1.2.17.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp2111848176638070524.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp2111848176638070524.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp2915826182393640288.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp2915826182393640288.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/11 17:53:47 INFO Executor: Fetching spark://ed3eecdcf213:44007/jars/com.101tec_zkclient-0.3.jar with timestamp 1681235625187\n",
            "23/04/11 17:53:47 INFO Utils: Fetching spark://ed3eecdcf213:44007/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp7933663231486743272.tmp\n",
            "23/04/11 17:53:47 INFO Utils: /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/fetchFileTemp7933663231486743272.tmp has been previously copied to /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 17:53:47 INFO Executor: Adding file:/tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/userFiles-141aafab-f264-47f1-8dc2-cd872ef97ae4/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/11 17:53:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37277.\n",
            "23/04/11 17:53:47 INFO NettyBlockTransferService: Server created on ed3eecdcf213:37277\n",
            "23/04/11 17:53:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/11 17:53:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ed3eecdcf213, 37277, None)\n",
            "23/04/11 17:53:47 INFO BlockManagerMasterEndpoint: Registering block manager ed3eecdcf213:37277 with 366.3 MiB RAM, BlockManagerId(driver, ed3eecdcf213, 37277, None)\n",
            "23/04/11 17:53:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ed3eecdcf213, 37277, None)\n",
            "23/04/11 17:53:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ed3eecdcf213, 37277, None)\n",
            "23/04/11 17:53:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/11 17:53:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/11 17:53:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ed3eecdcf213:37277 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:53:48 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/11 17:53:48 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/11 17:53:49 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/11 17:53:49 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 17:53:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 17:53:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 17:53:49 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/3_days.py:37) as input to shuffle 1\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Registering RDD 7 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/3_days.py:37), which has no missing parents\n",
            "23/04/11 17:53:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)\n",
            "23/04/11 17:53:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 365.9 MiB)\n",
            "23/04/11 17:53:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ed3eecdcf213:37277 (size: 7.6 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:53:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:53:49 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/3_days.py:37) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/11 17:53:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "23/04/11 17:53:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ed3eecdcf213, executor driver, partition 0, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:53:50 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ed3eecdcf213, executor driver, partition 1, PROCESS_LOCAL, 4499 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:53:50 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/11 17:53:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/11 17:53:50 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:12519625+12519626\n",
            "23/04/11 17:53:50 INFO HadoopRDD: Input split: file:/content/nashville-tweets-2019-01-28:0+12519625\n",
            "23/04/11 17:53:52 INFO PythonRunner: Times: total = 1157, boot = 645, init = 28, finish = 484\n",
            "23/04/11 17:53:52 INFO PythonRunner: Times: total = 1126, boot = 625, init = 39, finish = 462\n",
            "23/04/11 17:53:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1655 bytes result sent to driver\n",
            "23/04/11 17:53:52 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1612 bytes result sent to driver\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2333 ms on ed3eecdcf213 (executor driver) (1/2)\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2385 ms on ed3eecdcf213 (executor driver) (2/2)\n",
            "23/04/11 17:53:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:53:52 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 49107\n",
            "23/04/11 17:53:52 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/3_days.py:37) finished in 2.943 s\n",
            "23/04/11 17:53:52 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 17:53:52 INFO DAGScheduler: running: Set()\n",
            "23/04/11 17:53:52 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)\n",
            "23/04/11 17:53:52 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 17:53:52 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 17:53:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.6 KiB, free 365.9 MiB)\n",
            "23/04/11 17:53:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 365.9 MiB)\n",
            "23/04/11 17:53:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ed3eecdcf213:37277 (size: 6.2 KiB, free: 366.3 MiB)\n",
            "23/04/11 17:53:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:53:52 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "23/04/11 17:53:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (ed3eecdcf213, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (ed3eecdcf213, executor driver, partition 0, PROCESS_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:53:52 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "23/04/11 17:53:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 3)\n",
            "23/04/11 17:53:52 INFO ShuffleBlockFetcherIterator: Getting 2 (144.0 B) non-empty blocks including 2 (144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:53:52 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:53:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 36 ms\n",
            "23/04/11 17:53:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 33 ms\n",
            "23/04/11 17:53:52 INFO PythonRunner: Times: total = 26, boot = -884, init = 909, finish = 1\n",
            "23/04/11 17:53:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 3). 1654 bytes result sent to driver\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 206 ms on ed3eecdcf213 (executor driver) (1/2)\n",
            "23/04/11 17:53:52 INFO PythonRunner: Times: total = 68, boot = -878, init = 946, finish = 0\n",
            "23/04/11 17:53:52 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1783 bytes result sent to driver\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 263 ms on ed3eecdcf213 (executor driver) (2/2)\n",
            "23/04/11 17:53:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:53:52 INFO DAGScheduler: ShuffleMapStage 1 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.341 s\n",
            "23/04/11 17:53:52 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 17:53:52 INFO DAGScheduler: running: Set()\n",
            "23/04/11 17:53:52 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "23/04/11 17:53:52 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 17:53:52 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 17:53:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 105.9 KiB, free 365.8 MiB)\n",
            "23/04/11 17:53:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.7 MiB)\n",
            "23/04/11 17:53:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ed3eecdcf213:37277 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/11 17:53:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 17:53:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/11 17:53:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "23/04/11 17:53:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4) (ed3eecdcf213, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/11 17:53:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)\n",
            "23/04/11 17:53:53 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 17:53:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 17:53:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 17:53:53 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 17:53:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\n",
            "23/04/11 17:53:53 INFO PythonRunner: Times: total = 61, boot = -402, init = 463, finish = 0\n",
            "23/04/11 17:53:53 INFO FileOutputCommitter: Saved output of task 'attempt_202304111753491786148337591500640_0013_m_000000_0' to file:/content/3_days.out/_temporary/0/task_202304111753491786148337591500640_0013_m_000000\n",
            "23/04/11 17:53:53 INFO SparkHadoopMapRedUtil: attempt_202304111753491786148337591500640_0013_m_000000_0: Committed\n",
            "23/04/11 17:53:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1952 bytes result sent to driver\n",
            "23/04/11 17:53:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 443 ms on ed3eecdcf213 (executor driver) (1/1)\n",
            "23/04/11 17:53:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/11 17:53:53 INFO DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:83) finished in 0.518 s\n",
            "23/04/11 17:53:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/11 17:53:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "23/04/11 17:53:53 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 4.031658 s\n",
            "23/04/11 17:53:53 INFO SparkHadoopWriter: Start to commit write Job job_202304111753491786148337591500640_0013.\n",
            "23/04/11 17:53:53 INFO SparkHadoopWriter: Write Job job_202304111753491786148337591500640_0013 committed. Elapsed time: 45 ms.\n",
            "23/04/11 17:53:53 INFO SparkUI: Stopped Spark web UI at http://ed3eecdcf213:4040\n",
            "23/04/11 17:53:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/11 17:53:53 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/11 17:53:53 INFO BlockManager: BlockManager stopped\n",
            "23/04/11 17:53:53 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/11 17:53:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/11 17:53:53 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/11 17:53:54 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/11 17:53:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392\n",
            "23/04/11 17:53:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-8de6a0f8-0a1a-43ab-bd16-0827c2cd8392/pyspark-28dfbd5c-f935-42c1-ba94-28109cd82518\n",
            "23/04/11 17:53:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-d8c7a4ad-0b9b-4c0f-b93c-80458ea4dedf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h8y_YU2__D2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e8e4d0-3a5f-4136-c54f-cc9ee7ba82a2"
      },
      "source": [
        "def test3(lines):\n",
        "    if lines[0].strip() == \"('Sun', 6294)\":\n",
        "        print(\"passed\")\n",
        "    else:\n",
        "        assert False\n",
        "# test locall execution results\n",
        "with open('3_days.out/part-00000') as f:\n",
        "  lines = f.readlines()\n",
        "  test3(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdiDy-gu__D3"
      },
      "source": [
        "## Job 4. Join the batting and salaries data for Barry Bonds per year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MIbbmfk__D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f27b2e47-3d95-4ce6-ba77-b79120ccb01b"
      },
      "source": [
        "%%file 4_join.py\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import json\n",
        "from operator import add\n",
        "'''\n",
        "TODO:\n",
        "Join the batting and salaries data for Barry Bonds per year.\n",
        "\n",
        "The output should be the combined CSV string of batting and salaries data (one per year).\n",
        "\n",
        "Final output format:\n",
        "E.g:\n",
        "('join', 'bondsba01,2006,1,SFN,NL,130,367,74,99,23,0,26,77,3,0,115,51,38,10,0,1,92006,SFN,NL,bondsba01,19331470')\n",
        "\n",
        "Schema:\n",
        "Salaries: yearID\tteamID\tlgID\tplayerID\tsalary\n",
        "Batting: playerID\tyearID\tstint\tteamID\tlgID\tG\tAB\tR\tH\t2B\t3B\tHR\tRBI\tSB\tCS\tBB\tSO\n",
        "\n",
        "Hints: \n",
        "Use split to split the CSV lines (e.g. s = line.split(','))\n",
        "Both files are read as text file stream. Use the length of the lines to determine which is which.\n",
        "'''\n",
        "\n",
        "def process(line):\n",
        "    s = line.split(',')\n",
        "    if len(s) == 5:\n",
        "      playerID = s[3]\n",
        "      yearID = s[0]\n",
        "      if playerID == \"bondsba01\":\n",
        "        return (playerID, yearID), (\"salaries\", line) \n",
        "    else:\n",
        "      playerID = s[0]\n",
        "      yearID = s[1]\n",
        "      if playerID == \"bondsba01\":\n",
        "        # del s[-1] don't want to filter these out this time \n",
        "        # del s[5]\n",
        "        string = ','.join(s)\n",
        "        return (playerID, yearID), (\"batting\", string) \n",
        "    pass\n",
        "\n",
        "def barryBonds(line):\n",
        "    s = line.split(',')\n",
        "    if len(s) == 5:\n",
        "      playerID = s[3]\n",
        "      return playerID == \"bondsba01\"\n",
        "    else:\n",
        "      playerID = s[0]\n",
        "      return playerID == \"bondsba01\"\n",
        "\n",
        "def reducer(x, y):\n",
        "    output = \"\"\n",
        "    if x[0] == \"salaries\":\n",
        "      output = y[1] + x[1]\n",
        "    else:\n",
        "      output = x[1] + y[1]\n",
        "    return output\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    conf = SparkConf().setAppName('4_join').set('spark.hadoop.validateOutputSpecs', False)\n",
        "    sc = SparkContext(conf=conf).getOrCreate()\n",
        "    try:\n",
        "      #@todo: fix the path as required\n",
        "      # Read in the first CSV file\n",
        "      file1 = sc.textFile('s3://vandy-bd/hw6/Batting.csv')\n",
        "\n",
        "      # Read in the second CSV file\n",
        "      file2 = sc.textFile('s3://vandy-bd/hw6/Salaries.csv')\n",
        "\n",
        "      # Union the two RDDs\n",
        "      merged_rdd = file1.union(file2)\n",
        "\n",
        "      filtered = merged_rdd.filter(lambda x: barryBonds(x))\n",
        "      # review the page rank example for how to use the map operation\n",
        "      # review word count for reduce and add\n",
        "      # see how we use map to parse each row\n",
        "      info = filtered.map(lambda line: process(line))\n",
        "\n",
        "      # @todo: create an output rdd that uses reduce by key and add operator to add up the correct entries and incorrect entries\n",
        "      reduced = info.reduceByKey(lambda x, y: reducer(x,y))\n",
        "\n",
        "      output = reduced.map(lambda x: ('join', x[1]))\n",
        "      # @todo: the s3 version will have to save it to correct s3 path\n",
        "      output.repartition(1).saveAsTextFile(\"s3://vandy-bd/hw6/4_join.out\")\n",
        "\n",
        "    finally:\n",
        "      # very important: stop the context. Otherwise you may get an error that context is still alive. if you are on colab just restart the runtime if you face problem\n",
        "      #finally is used to make sure the context is stopped even with errors\n",
        "      sc.stop()\n",
        "\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 4_join.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5_a0GAy__D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e564157-085e-4227-885d-bc45e7945f22"
      },
      "source": [
        "# execute locally\n",
        "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 4_join.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/content/spark-3.2.3-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-789dddd2-0248-4cd2-aebe-4e39c0cc94b3;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 in central\n",
            "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
            "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
            "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
            "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 in central\n",
            "\tfound com.101tec#zkclient;0.3 in central\n",
            "\tfound log4j#log4j;1.2.17 in central\n",
            "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
            "\tfound net.jpountz.lz4#lz4;1.2.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            ":: resolution report :: resolve 1152ms :: artifacts dl 20ms\n",
            "\t:: modules in use:\n",
            "\tcom.101tec#zkclient;0.3 from central in [default]\n",
            "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
            "\tlog4j#log4j;1.2.17 from central in [default]\n",
            "\tnet.jpountz.lz4#lz4;1.2.0 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
            "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.4.7 from central in [default]\n",
            "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.1.0 from central in [default]\n",
            "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-789dddd2-0248-4cd2-aebe-4e39c0cc94b3\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 12 already retrieved (0kB/14ms)\n",
            "23/04/11 18:18:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/04/11 18:18:58 INFO SparkContext: Running Spark version 3.2.3\n",
            "23/04/11 18:18:58 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 18:18:58 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/04/11 18:18:58 INFO ResourceUtils: ==============================================================\n",
            "23/04/11 18:18:58 INFO SparkContext: Submitted application: 4_join\n",
            "23/04/11 18:18:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/04/11 18:18:58 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/04/11 18:18:58 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/04/11 18:18:59 INFO SecurityManager: Changing view acls to: root\n",
            "23/04/11 18:18:59 INFO SecurityManager: Changing modify acls to: root\n",
            "23/04/11 18:18:59 INFO SecurityManager: Changing view acls groups to: \n",
            "23/04/11 18:18:59 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/04/11 18:18:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/04/11 18:18:59 INFO Utils: Successfully started service 'sparkDriver' on port 41855.\n",
            "23/04/11 18:18:59 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/04/11 18:18:59 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/04/11 18:18:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/04/11 18:18:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/04/11 18:18:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/11 18:18:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7219cf5a-8e7f-48c9-9bb2-81e7ffa87911\n",
            "23/04/11 18:18:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/04/11 18:18:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/11 18:19:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "23/04/11 18:19:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ed3eecdcf213:4040\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at spark://ed3eecdcf213:41855/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://ed3eecdcf213:41855/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://ed3eecdcf213:41855/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at spark://ed3eecdcf213:41855/jars/com.101tec_zkclient-0.3.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://ed3eecdcf213:41855/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://ed3eecdcf213:41855/jars/log4j_log4j-1.2.17.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at spark://ed3eecdcf213:41855/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://ed3eecdcf213:41855/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar at file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar at file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar at file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar at file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/log4j_log4j-1.2.17.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/log4j_log4j-1.2.17.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 18:19:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:00 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 18:19:01 INFO Executor: Starting executor ID driver on host ed3eecdcf213\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/log4j_log4j-1.2.17.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/log4j_log4j-1.2.17.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/log4j_log4j-1.2.17.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/net.jpountz.lz4_lz4-1.2.0.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/com.101tec_zkclient-0.3.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/com.101tec_zkclient-0.3.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching file:///root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: /root/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/net.jpountz.lz4_lz4-1.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO TransportClientFactory: Successfully created connection to ed3eecdcf213/172.28.0.12:41855 after 61 ms (0 ms spent in bootstraps)\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/net.jpountz.lz4_lz4-1.2.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp7214593820985547119.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp7214593820985547119.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/net.jpountz.lz4_lz4-1.2.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/net.jpountz.lz4_lz4-1.2.0.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/log4j_log4j-1.2.17.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/log4j_log4j-1.2.17.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp3576772516628083415.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp3576772516628083415.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/log4j_log4j-1.2.17.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/log4j_log4j-1.2.17.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka-clients-0.8.2.1.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp4163802919084522420.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp4163802919084522420.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka-clients-0.8.2.1.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka-clients-0.8.2.1.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp3106943651923066360.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp3106943651923066360.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.yammer.metrics_metrics-core-2.2.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.yammer.metrics_metrics-core-2.2.0.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/com.101tec_zkclient-0.3.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/com.101tec_zkclient-0.3.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp2820697503579822926.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp2820697503579822926.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.101tec_zkclient-0.3.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/com.101tec_zkclient-0.3.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5743389961248549584.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5743389961248549584.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.spark-project.spark_unused-1.0.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.spark-project.spark_unused-1.0.0.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp1148893212453496091.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp1148893212453496091.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp6145049119202286918.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp6145049119202286918.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.xerial.snappy_snappy-java-1.1.7.5.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5474905512289250990.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5474905512289250990.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.slf4j_slf4j-api-1.7.16.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp655982983456073997.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp655982983456073997.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.spark_spark-streaming-kafka-0-8_2.11-2.4.7.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.apache.kafka_kafka_2.11-0.8.2.1.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp4201345403417177334.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp4201345403417177334.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.apache.kafka_kafka_2.11-0.8.2.1.jar to class loader\n",
            "23/04/11 18:19:01 INFO Executor: Fetching spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar with timestamp 1681237138848\n",
            "23/04/11 18:19:01 INFO Utils: Fetching spark://ed3eecdcf213:41855/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5709645794656824965.tmp\n",
            "23/04/11 18:19:01 INFO Utils: /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/fetchFileTemp5709645794656824965.tmp has been previously copied to /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar\n",
            "23/04/11 18:19:01 INFO Executor: Adding file:/tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/userFiles-bd2ccfe7-9a53-4ea3-8479-5088275ad4a8/org.scala-lang.modules_scala-parser-combinators_2.11-1.1.0.jar to class loader\n",
            "23/04/11 18:19:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32799.\n",
            "23/04/11 18:19:01 INFO NettyBlockTransferService: Server created on ed3eecdcf213:32799\n",
            "23/04/11 18:19:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/04/11 18:19:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ed3eecdcf213, 32799, None)\n",
            "23/04/11 18:19:01 INFO BlockManagerMasterEndpoint: Registering block manager ed3eecdcf213:32799 with 366.3 MiB RAM, BlockManagerId(driver, ed3eecdcf213, 32799, None)\n",
            "23/04/11 18:19:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ed3eecdcf213, 32799, None)\n",
            "23/04/11 18:19:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ed3eecdcf213, 32799, None)\n",
            "23/04/11 18:19:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 354.3 KiB, free 366.0 MiB)\n",
            "23/04/11 18:19:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.9 MiB)\n",
            "23/04/11 18:19:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ed3eecdcf213:32799 (size: 32.0 KiB, free: 366.3 MiB)\n",
            "23/04/11 18:19:02 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/11 18:19:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 354.3 KiB, free 365.6 MiB)\n",
            "23/04/11 18:19:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.0 KiB, free 365.5 MiB)\n",
            "23/04/11 18:19:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ed3eecdcf213:32799 (size: 32.0 KiB, free: 366.2 MiB)\n",
            "23/04/11 18:19:03 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/04/11 18:19:03 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/11 18:19:03 INFO FileInputFormat: Total input files to process : 1\n",
            "23/04/11 18:19:03 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/04/11 18:19:03 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 18:19:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 18:19:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 18:19:03 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Registering RDD 6 (reduceByKey at /content/4_join.py:78) as input to shuffle 1\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Registering RDD 10 (coalesce at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
            "23/04/11 18:19:03 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[6] at reduceByKey at /content/4_join.py:78), which has no missing parents\n",
            "23/04/11 18:19:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.0 KiB, free 365.5 MiB)\n",
            "23/04/11 18:19:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 365.5 MiB)\n",
            "23/04/11 18:19:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ed3eecdcf213:32799 (size: 8.4 KiB, free: 366.2 MiB)\n",
            "23/04/11 18:19:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 18:19:04 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 0 (PairwiseRDD[6] at reduceByKey at /content/4_join.py:78) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "23/04/11 18:19:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks resource profile 0\n",
            "23/04/11 18:19:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ed3eecdcf213, executor driver, partition 0, PROCESS_LOCAL, 4592 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:04 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ed3eecdcf213, executor driver, partition 1, PROCESS_LOCAL, 4592 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/04/11 18:19:04 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "23/04/11 18:19:04 INFO HadoopRDD: Input split: file:/content/Batting.csv:3244373+3244374\n",
            "23/04/11 18:19:04 INFO HadoopRDD: Input split: file:/content/Batting.csv:0+3244373\n",
            "23/04/11 18:19:06 INFO PythonRunner: Times: total = 1135, boot = 680, init = 43, finish = 412\n",
            "23/04/11 18:19:06 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1485 bytes result sent to driver\n",
            "23/04/11 18:19:06 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (ed3eecdcf213, executor driver, partition 2, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:06 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
            "23/04/11 18:19:06 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1957 ms on ed3eecdcf213 (executor driver) (1/4)\n",
            "23/04/11 18:19:06 INFO HadoopRDD: Input split: file:/content/Salaries.csv:0+350012\n",
            "23/04/11 18:19:06 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55037\n",
            "23/04/11 18:19:06 INFO PythonRunner: Times: total = 83, boot = -108, init = 124, finish = 67\n",
            "23/04/11 18:19:06 INFO PythonRunner: Times: total = 1205, boot = 673, init = 68, finish = 464\n",
            "23/04/11 18:19:07 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1571 bytes result sent to driver\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (ed3eecdcf213, executor driver, partition 3, PROCESS_LOCAL, 4593 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:07 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
            "23/04/11 18:19:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1571 bytes result sent to driver\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 883 ms on ed3eecdcf213 (executor driver) (2/4)\n",
            "23/04/11 18:19:07 INFO HadoopRDD: Input split: file:/content/Salaries.csv:350012+350012\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2871 ms on ed3eecdcf213 (executor driver) (3/4)\n",
            "23/04/11 18:19:07 INFO PythonRunner: Times: total = 139, boot = -712, init = 734, finish = 117\n",
            "23/04/11 18:19:07 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1614 bytes result sent to driver\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 381 ms on ed3eecdcf213 (executor driver) (4/4)\n",
            "23/04/11 18:19:07 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/4_join.py:78) finished in 3.803 s\n",
            "23/04/11 18:19:07 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 18:19:07 INFO DAGScheduler: running: Set()\n",
            "23/04/11 18:19:07 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)\n",
            "23/04/11 18:19:07 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 18:19:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/04/11 18:19:07 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[10] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 18:19:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.1 KiB, free 365.5 MiB)\n",
            "23/04/11 18:19:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)\n",
            "23/04/11 18:19:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ed3eecdcf213:32799 (size: 6.5 KiB, free: 366.2 MiB)\n",
            "23/04/11 18:19:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 18:19:07 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[10] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
            "23/04/11 18:19:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4) (ed3eecdcf213, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5) (ed3eecdcf213, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 4)\n",
            "23/04/11 18:19:07 INFO Executor: Running task 1.0 in stage 1.0 (TID 5)\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Getting 3 (1152.0 B) non-empty blocks including 3 (1152.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 33 ms\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Getting 3 (717.0 B) non-empty blocks including 3 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms\n",
            "23/04/11 18:19:07 INFO PythonRunner: Times: total = 37, boot = -1614, init = 1650, finish = 1\n",
            "23/04/11 18:19:07 INFO Executor: Finished task 1.0 in stage 1.0 (TID 5). 1783 bytes result sent to driver\n",
            "23/04/11 18:19:07 INFO PythonRunner: Times: total = 57, boot = -541, init = 598, finish = 0\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6) (ed3eecdcf213, executor driver, partition 2, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 294 ms on ed3eecdcf213 (executor driver) (1/4)\n",
            "23/04/11 18:19:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 4). 1783 bytes result sent to driver\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7) (ed3eecdcf213, executor driver, partition 3, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 330 ms on ed3eecdcf213 (executor driver) (2/4)\n",
            "23/04/11 18:19:07 INFO Executor: Running task 3.0 in stage 1.0 (TID 7)\n",
            "23/04/11 18:19:07 INFO Executor: Running task 2.0 in stage 1.0 (TID 6)\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Getting 3 (723.0 B) non-empty blocks including 3 (723.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Getting 3 (817.0 B) non-empty blocks including 3 (817.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 18:19:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
            "23/04/11 18:19:08 INFO PythonRunner: Times: total = 49, boot = -31, init = 80, finish = 0\n",
            "23/04/11 18:19:08 INFO Executor: Finished task 3.0 in stage 1.0 (TID 7). 1783 bytes result sent to driver\n",
            "23/04/11 18:19:08 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 127 ms on ed3eecdcf213 (executor driver) (3/4)\n",
            "23/04/11 18:19:08 INFO PythonRunner: Times: total = 49, boot = -33, init = 82, finish = 0\n",
            "23/04/11 18:19:08 INFO Executor: Finished task 2.0 in stage 1.0 (TID 6). 1783 bytes result sent to driver\n",
            "23/04/11 18:19:08 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 166 ms on ed3eecdcf213 (executor driver) (4/4)\n",
            "23/04/11 18:19:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/04/11 18:19:08 INFO DAGScheduler: ShuffleMapStage 1 (coalesce at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
            "23/04/11 18:19:08 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/04/11 18:19:08 INFO DAGScheduler: running: Set()\n",
            "23/04/11 18:19:08 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
            "23/04/11 18:19:08 INFO DAGScheduler: failed: Set()\n",
            "23/04/11 18:19:08 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[16] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/04/11 18:19:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 105.9 KiB, free 365.4 MiB)\n",
            "23/04/11 18:19:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 365.4 MiB)\n",
            "23/04/11 18:19:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ed3eecdcf213:32799 (size: 39.6 KiB, free: 366.2 MiB)\n",
            "23/04/11 18:19:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
            "23/04/11 18:19:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[16] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/04/11 18:19:08 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "23/04/11 18:19:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (ed3eecdcf213, executor driver, partition 0, NODE_LOCAL, 4547 bytes) taskResourceAssignments Map()\n",
            "23/04/11 18:19:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 8)\n",
            "23/04/11 18:19:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/04/11 18:19:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/04/11 18:19:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/04/11 18:19:08 INFO ShuffleBlockFetcherIterator: Getting 4 (2.1 KiB) non-empty blocks including 4 (2.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "23/04/11 18:19:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
            "23/04/11 18:19:08 INFO PythonRunner: Times: total = 58, boot = -360, init = 418, finish = 0\n",
            "23/04/11 18:19:08 INFO FileOutputCommitter: Saved output of task 'attempt_202304111819036714100715957713928_0016_m_000000_0' to file:/content/4_join.out/_temporary/0/task_202304111819036714100715957713928_0016_m_000000\n",
            "23/04/11 18:19:08 INFO SparkHadoopMapRedUtil: attempt_202304111819036714100715957713928_0016_m_000000_0: Committed\n",
            "23/04/11 18:19:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 8). 1952 bytes result sent to driver\n",
            "23/04/11 18:19:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 435 ms on ed3eecdcf213 (executor driver) (1/1)\n",
            "23/04/11 18:19:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "23/04/11 18:19:08 INFO DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:83) finished in 0.548 s\n",
            "23/04/11 18:19:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/04/11 18:19:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "23/04/11 18:19:08 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 5.110313 s\n",
            "23/04/11 18:19:08 INFO SparkHadoopWriter: Start to commit write Job job_202304111819036714100715957713928_0016.\n",
            "23/04/11 18:19:08 INFO SparkHadoopWriter: Write Job job_202304111819036714100715957713928_0016 committed. Elapsed time: 32 ms.\n",
            "23/04/11 18:19:08 INFO SparkUI: Stopped Spark web UI at http://ed3eecdcf213:4040\n",
            "23/04/11 18:19:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/04/11 18:19:08 INFO MemoryStore: MemoryStore cleared\n",
            "23/04/11 18:19:08 INFO BlockManager: BlockManager stopped\n",
            "23/04/11 18:19:08 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/04/11 18:19:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/04/11 18:19:08 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/04/11 18:19:09 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/04/11 18:19:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969\n",
            "23/04/11 18:19:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-501c2b0c-bdf0-49da-a0a6-f66141b35969/pyspark-e2e7f5af-c9de-4c72-a442-c3a092711e96\n",
            "23/04/11 18:19:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-64066caf-9c9b-418d-97b6-708eb32de1e7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpkDNQCb__D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a66a4f5-9d64-4f3a-da7f-7b7357b58940"
      },
      "source": [
        "# test locally\n",
        "results = [('join', 'bondsba01,1991,1,PIT,NL,153,153,510,95,149,28,5,25,116,43,13,107,73,25,4,0,13,8,1531991,PIT,NL,bondsba01,2300000'),\n",
        "('join', 'bondsba01,1993,1,SFN,NL,159,159,539,129,181,38,4,46,123,29,12,126,79,43,2,0,7,11,1591993,SFN,NL,bondsba01,4516666'),\n",
        "('join', 'bondsba01,2002,1,SFN,NL,143,143,403,117,149,31,2,46,110,9,2,198,47,68,9,0,2,4,1432002,SFN,NL,bondsba01,15000000'),\n",
        "('join', 'bondsba01,2004,1,SFN,NL,147,147,373,129,135,27,3,45,101,6,1,232,41,120,9,0,3,5,1472004,SFN,NL,bondsba01,18000000'),\n",
        "('join', 'bondsba01,1986,1,PIT,NL,113,113,413,72,92,26,3,16,48,36,7,65,102,2,2,2,2,4,1131986,PIT,NL,bondsba01,60000'),\n",
        "('join', 'bondsba01,1996,1,SFN,NL,158,158,517,122,159,27,3,42,129,40,7,151,76,30,1,0,6,11,1581996,SFN,NL,bondsba01,8416667'),\n",
        "('join', 'bondsba01,1997,1,SFN,NL,159,159,532,123,155,26,5,40,101,37,8,145,87,34,8,0,5,13,1591997,SFN,NL,bondsba01,8666667'),\n",
        "('join', 'bondsba01,1999,1,SFN,NL,102,102,355,91,93,20,2,34,83,15,2,73,62,9,3,0,3,6,1021999,SFN,NL,bondsba01,9381057'),\n",
        "('join', 'bondsba01,1990,1,PIT,NL,151,151,519,104,156,32,3,33,114,52,13,93,83,15,3,0,6,8,1511990,PIT,NL,bondsba01,850000'),\n",
        "('join', 'bondsba01,1994,1,SFN,NL,112,112,391,89,122,18,1,37,81,29,9,74,43,18,6,0,3,3,1121994,SFN,NL,bondsba01,5166666'),\n",
        "('join', 'bondsba01,1995,1,SFN,NL,144,144,506,109,149,30,7,33,104,31,10,120,83,22,5,0,4,12,1441995,SFN,NL,bondsba01,8166666'),\n",
        "('join', 'bondsba01,2003,1,SFN,NL,130,130,390,111,133,22,1,45,90,7,0,148,58,61,10,0,2,7,1302003,SFN,NL,bondsba01,15500000'),\n",
        "('join', 'bondsba01,2007,1,SFN,NL,126,126,340,75,94,14,0,28,66,5,0,132,54,43,3,0,2,13,1262007,SFN,NL,bondsba01,15533970'),\n",
        "('join', 'bondsba01,1987,1,PIT,NL,150,150,551,99,144,34,9,25,59,32,10,54,88,3,3,0,3,4,1501987,PIT,NL,bondsba01,100000'),\n",
        "('join', 'bondsba01,1988,1,PIT,NL,144,144,538,97,152,30,5,24,58,17,11,72,82,14,2,0,2,3,1441988,PIT,NL,bondsba01,220000'),\n",
        "('join', 'bondsba01,1989,1,PIT,NL,159,159,580,96,144,34,6,19,58,32,10,93,93,22,1,1,4,9,1591989,PIT,NL,bondsba01,360000'),\n",
        "('join', 'bondsba01,1992,1,PIT,NL,140,140,473,109,147,36,5,34,103,39,8,127,69,32,5,0,7,9,1401992,PIT,NL,bondsba01,4800000'),\n",
        "('join', 'bondsba01,1998,1,SFN,NL,156,156,552,120,167,44,7,37,122,28,12,130,92,29,8,1,6,15,1561998,SFN,NL,bondsba01,8916667'),\n",
        "('join', 'bondsba01,2000,1,SFN,NL,143,143,480,129,147,28,4,49,106,11,3,117,77,22,3,0,7,6,1432000,SFN,NL,bondsba01,10658826'),\n",
        "('join', 'bondsba01,2001,1,SFN,NL,153,153,476,129,156,32,2,73,137,13,3,177,93,35,9,0,2,5,1532001,SFN,NL,bondsba01,10300000'),\n",
        "('join', 'bondsba01,2005,1,SFN,NL,14,14,42,8,12,1,0,5,10,0,0,9,6,3,0,0,1,0,142005,SFN,NL,bondsba01,22000000'),\n",
        "('join', 'bondsba01,2006,1,SFN,NL,130,130,367,74,99,23,0,26,77,3,0,115,51,38,10,0,1,9,1302006,SFN,NL,bondsba01,19331470')]\n",
        "def test4(lines):\n",
        "  global results\n",
        "  results = [str(x) for x in results]\n",
        "  find_lines = 0\n",
        "  for  line in lines:\n",
        "    if line.strip() in results:\n",
        "        find_lines += 1\n",
        "  if find_lines != 22:\n",
        "      assert False\n",
        "  print('test passed')\n",
        "with open('4_join.out/part-00000') as f:\n",
        "    lines = f.readlines()\n",
        "    test4(lines)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 Execute the same scripts on EMR.\n",
        "\n",
        "* Make sure that you have created an EMR cluster using the instructions in the main readme.\n",
        "* upload the main data to s3"
      ],
      "metadata": {
        "id": "Ed2gw0o387bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup AWS Credentials and fill them here. Make sure you do not save this information back to github"
      ],
      "metadata": {
        "id": "dyeTsQLV-B-u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMEsJXTU__Dx"
      },
      "source": [
        "# Please fill your aws credential information here\n",
        "credentials = {\n",
        "    'region_name': 'us-east-1',\n",
        "    'aws_access_key_id': 'ASIA6CQF7SPNOL6UAOFJ',\n",
        "    'aws_secret_access_key': 'cMMvDZh/1hPipMSJlgB5hX0Nf3Iy2dXa+hE6Ek7C',\n",
        "    'aws_session_token': 'FwoGZXIvYXdzENP//////////wEaDIXDcykaqQX8xRTKOSLOAZnctufpKy0ahlGtM/9i4U2eDWRFOvaG8Xj8gjTtcI+zdH3E8FkD4lfKFKJxKAbqtTGqTvkMF02coCAiaO6eoAX9jhiPwPEzZKgVHjJFBYw4vaHHtBzCvYhi2sOHrFc2BTpHAEhlOEwAmk1MA0OoTZ9JPJo4bVfQfiiHCmpqVxZmSf5FB/lip5rN/bh+QdmH3TlOU1+bQnRV+a5Ixdd3TsEgm0Np7FDE7SvjcQ2G9rA2YRP+iIuIo9YS5OdiJQvmKUALYt8BmRmrUVdLR3l7KOCu1qEGMi2hylSI02FS7J7cHkRPxtFifq0aJohaczxBFBRvytWQOgviikIpvwBd33/slFs='\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRJjVwtO__Dx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10d2b50-bf94-4dee-e3e8-46ea4a5bf807"
      },
      "source": [
        "!pip install boto3\n",
        "import boto3, json\n",
        "\n",
        "session = boto3.session.Session(**credentials)\n",
        "s3 = session.client('s3')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.110-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.110\n",
            "  Downloading botocore-1.29.110-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.110->boto3) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.110->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.110->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.26.110 botocore-1.29.110 jmespath-1.0.1 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data to S3"
      ],
      "metadata": {
        "id": "Mss_SpU89_eo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-9aO5-hmAv4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MzsIwij__Dy"
      },
      "source": [
        "# upload tweets dataset to S3, please replace the bucket name and object keys with yours\n",
        "s3.upload_file(Filename='nashville-tweets-2019-01-28', Bucket='vandy-bd', Key='hw6/nashville-tweets-2019-01-28')\n",
        "s3.upload_file(Filename='Batting.csv', Bucket='vandy-bd', Key='hw6/Batting.csv')\n",
        "s3.upload_file(Filename='Salaries.csv', Bucket='vandy-bd', Key='hw6/Salaries.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lljwHyOy__D2"
      },
      "source": [
        "# replae with your EMR cluster ID\n",
        "CLUSTER_ID = 'j-145LUR2LF753J'\n",
        "\n",
        "def submit_job(app_name, pyfile_uri):\n",
        "    emr = session.client('emr')\n",
        "    emr.add_job_flow_steps(JobFlowId=CLUSTER_ID, Steps=[{\n",
        "        'Name': app_name,\n",
        "        'ActionOnFailure': 'CANCEL_AND_WAIT',\n",
        "        'HadoopJarStep': {\n",
        "            'Args': ['spark-submit',\n",
        "                     '--master', 'yarn',\n",
        "                     '--deploy-mode', 'cluster',\n",
        "                     pyfile_uri],\n",
        "            'Jar': 'command-runner.jar'\n",
        "        }}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzOGzTj__D1"
      },
      "source": [
        "# upload script to S3\n",
        "s3.upload_file(Filename='2_group.py', Bucket='vandy-bd', Key='hw6/2_group.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoqkfcEn__D1"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='2_group', pyfile_uri='s3://vandy-bd/hw6/2_group.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm7iAdlL__D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35f2d19-d43c-4ae8-efc8-433bb26ed76b"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/2_group.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bd', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test2(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VUV5ZIp__D2"
      },
      "source": [
        "# upload script to S3 - Make sure that the S3 bucket name is changed to your own bucket\n",
        "s3.upload_file(Filename='3_days.py', Bucket='vandy-bd', Key='hw6/3_days.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFWrjexF__D2"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='3_days', pyfile_uri='s3://vandy-bd/hw6/3_days.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1eFnxXZ__D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f192a253-314b-4678-c9e7-db3fdac0781c"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/3_days.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bd', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test3(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload script to S3 - Make sure that the S3 bucket name is changed to your own bucket\n",
        "s3.upload_file(Filename='4_join.py', Bucket='vandy-bd', Key='hw6/4_join.py')"
      ],
      "metadata": {
        "id": "Xxf7WSbSDcsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkQiNiXW__D4"
      },
      "source": [
        "# submit spark job to emr\n",
        "submit_job(app_name='4_join', pyfile_uri='s3://vandy-bd/hw6/4_join.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqyNRexw__D4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5aa5302-ae2d-4ab0-bc77-a6a4cc5e430a"
      },
      "source": [
        "# test emr execution results\n",
        "output_key = \"hw6/4_join.out/part-00000\"\n",
        "lines = s3.get_object(Bucket='vandy-bd', Key=output_key)['Body'].read().decode().splitlines()\n",
        "test4(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test passed\n"
          ]
        }
      ]
    }
  ]
}